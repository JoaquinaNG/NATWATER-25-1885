{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d64798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b105b",
   "metadata": {},
   "source": [
    "# Converting Node Observations into Network Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc19a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = Path(\"..\") / \"Data\" / \"SurveyData.xlsx\"\n",
    "\n",
    "# Load raw (unprocessed) data from Excel using correct sheet names\n",
    "raw_network_data = {\n",
    "    'TribA': {\n",
    "        'nodes': pd.read_excel(PATH_TO_DATA, sheet_name=\"TribA_Survey_Data\", header=0),\n",
    "        'confluences': pd.read_excel(PATH_TO_DATA, sheet_name=\"TribA_Ghost_Nodes\"),\n",
    "        'stretches': pd.read_excel(PATH_TO_DATA, sheet_name=\"TribA_Edges\").astype(str)\n",
    "    },\n",
    "    'TribB': {\n",
    "        'nodes': pd.read_excel(PATH_TO_DATA, sheet_name=\"TribB_Survey_Data\", header=0),\n",
    "        'confluences': pd.read_excel(PATH_TO_DATA, sheet_name=\"TribB_Ghost_Nodes\"),\n",
    "        'stretches': pd.read_excel(PATH_TO_DATA, sheet_name=\"TribB_Edges\").astype(str)\n",
    "    },\n",
    "    'TribC': {\n",
    "        'nodes': pd.read_excel(PATH_TO_DATA, sheet_name=\"TribC_Survey_Data\", header=0),\n",
    "        'confluences': pd.read_excel(PATH_TO_DATA, sheet_name=\"TribC_Ghost_Nodes\"),\n",
    "        'stretches': pd.read_excel(PATH_TO_DATA, sheet_name=\"TribC_Edges\").astype(str)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef94493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how to convert survey values to boolean\n",
    "def convert_to_bool_nodes(df):\n",
    "    df_bool = df.copy()\n",
    "    survey_cols = [col for col in df.columns if \"202\" in col]  # Detect date columns\n",
    "    for col in survey_cols:\n",
    "        df_bool[col] = df[col].map({\n",
    "            \"Yes\": True,\n",
    "            \"No\": False\n",
    "        })\n",
    "    return df_bool\n",
    "\n",
    "# Create a new dictionary with boolean-processed 'nodes'\n",
    "bool_network_data = {}\n",
    "\n",
    "for trib, data in raw_network_data.items():\n",
    "    bool_network_data[trib] = {\n",
    "        'nodes': convert_to_bool_nodes(data['nodes']),\n",
    "        'confluences': data['confluences'],\n",
    "        'stretches': data['stretches']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786a2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_network(survey_date, nodes_data_bool, confluence_nodes, stretches, trib_key):\n",
    "    # Ensure the None values are correctly registered\n",
    "    nodes_data_bool = nodes_data_bool.where(nodes_data_bool.notnull(), None)\n",
    "\n",
    "    # Create directed network\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Create nodes and add the attribute corresponding to data\n",
    "    for _, data in nodes_data_bool.iterrows():\n",
    "        node_ID = str(data[\"Point ID\"])\n",
    "        G.add_node(node_ID, pos=(data['Longitude'], data['Latitude']), node_activity=data[survey_date])\n",
    "\n",
    "    # Add confluence nodes\n",
    "    for _, row in confluence_nodes.iterrows():\n",
    "        G.add_node(row['Node'], pos=(row['Longitude'], row['Latitude']), node_activity=None)  # Set a default value\n",
    "\n",
    "    # Explicitly set activity for a necessary node (e.g., those that correspond to Little Calumet River)\n",
    "    if trib_key == 'TribB':\n",
    "        g6_node = \"g6\"  \n",
    "        if g6_node in G.nodes:\n",
    "            G.nodes[g6_node][\"node_activity\"] = True\n",
    "\n",
    "    # Add network edges // river stretches\n",
    "    for _, row in stretches.iterrows():\n",
    "        if row[\"Node Start\"] in G.nodes and row[\"Node End\"] in G.nodes:\n",
    "            G.add_edge(row[\"Node Start\"], row[\"Node End\"])\n",
    "\n",
    "    # Definitions to calculate the value of each node\n",
    "    def find_upstream(node_ID):\n",
    "        if G.nodes[node_ID].get(\"node_activity\", None) is not None:\n",
    "            return G.nodes[node_ID][\"node_activity\"]\n",
    "        \n",
    "        is_upstream_wet = None\n",
    "        for up_index, _ in G.in_edges(node_ID):\n",
    "            res = find_upstream(up_index)\n",
    "            if res is not None:\n",
    "                is_upstream_wet = is_upstream_wet or res\n",
    "        return is_upstream_wet\n",
    "        \n",
    "    def find_downstream(node_ID):\n",
    "        if G.nodes[node_ID].get(\"node_activity\", None) is not None:\n",
    "            return G.nodes[node_ID][\"node_activity\"]\n",
    "        \n",
    "        is_downstream_wet = None\n",
    "        for _, down_index in G.out_edges(node_ID):\n",
    "            res = find_downstream(down_index)\n",
    "            if res is not None:\n",
    "                is_downstream_wet = is_downstream_wet or res\n",
    "        return is_downstream_wet\n",
    "\n",
    "    # First pass: Initial assignment of node activity\n",
    "    for node_ID in G.nodes:\n",
    "        upstream = find_upstream(node_ID)\n",
    "        downstream = find_downstream(node_ID)\n",
    "        if upstream == downstream:\n",
    "            G.nodes[node_ID][\"node_activity\"] = upstream\n",
    "            \n",
    "    # # Check that all nodes have been assigned\n",
    "    # for node_ID in G.nodes:\n",
    "    #     node_attrs = G.nodes[node_ID]\n",
    "    #     if node_attrs.get(\"node_activity\", None) is None:\n",
    "    #         print(node_ID)\n",
    "\n",
    "    # Second pass: Assign \"disconnected\" or \"indeterminate\"\n",
    "    for node_ID in G.nodes:\n",
    "        if G.nodes[node_ID][\"node_activity\"] is None:\n",
    "            upstream = find_upstream(node_ID)\n",
    "            downstream = find_downstream(node_ID)\n",
    "\n",
    "            if upstream is not None and downstream is not None:\n",
    "                if upstream != downstream:\n",
    "                    G.nodes[node_ID][\"node_activity\"] = \"disconnected\"\n",
    "            else:\n",
    "                G.nodes[node_ID][\"node_activity\"] = None\n",
    "\n",
    "\n",
    "    # Assign stretch attribute based on node activity\n",
    "    for up_index, down_index in G.edges:\n",
    "        up_activity = G.nodes[up_index].get(\"node_activity\")\n",
    "        down_activity = G.nodes[down_index].get(\"node_activity\")\n",
    "        \n",
    "        if up_activity is True and down_activity is True:\n",
    "            G.edges[up_index, down_index][\"color\"] = 'blue'\n",
    "        elif up_activity is False and down_activity is False:\n",
    "            G.edges[up_index, down_index][\"color\"] = 'red'\n",
    "        elif up_activity is True and down_activity is False:\n",
    "            G.edges[up_index, down_index][\"color\"] = 'orange'\n",
    "        elif up_activity is False and down_activity is True:\n",
    "            G.edges[up_index, down_index][\"color\"] = 'orange'\n",
    "        elif \"disconnected\" in (up_activity, down_activity):\n",
    "            G.edges[up_index, down_index][\"color\"] = 'orange'\n",
    "        else:\n",
    "            G.edges[up_index, down_index][\"color\"] = 'grey'\n",
    "\n",
    "    # Assign activity to stretches\n",
    "    for up_index, down_index in G.edges:\n",
    "        up_activity = G.nodes[up_index].get(\"node_activity\")\n",
    "        down_activity = G.nodes[down_index].get(\"node_activity\")\n",
    "        \n",
    "        if up_activity is True and down_activity is True:\n",
    "            G.edges[up_index, down_index][\"stretch_activity\"] = 'Active'\n",
    "        elif up_activity is False and down_activity is False:\n",
    "            G.edges[up_index, down_index][\"stretch_activity\"] = 'Not Active'\n",
    "        elif up_activity is True and down_activity is False:\n",
    "            G.edges[up_index, down_index][\"stretch_activity\"] = 'Disconnected'\n",
    "        elif up_activity is False and down_activity is True:\n",
    "            G.edges[up_index, down_index][\"stretch_activity\"] = 'Disconnected'\n",
    "        elif \"disconnected\" in (up_activity, down_activity):\n",
    "            G.edges[up_index, down_index][\"stretch_activity\"] = 'Disconnected'\n",
    "        else:\n",
    "            G.edges[up_index, down_index][\"stretch_activity\"] = 'Not enough information'\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478fdeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_stretch_activities(trib_key, survey_dates, bool_network_data):\n",
    "    # Extract the relevant data for the tributary\n",
    "    nodes_data_bool = bool_network_data[trib_key]['nodes']\n",
    "    confluence_nodes = bool_network_data[trib_key]['confluences']\n",
    "    stretches = bool_network_data[trib_key]['stretches']\n",
    "    \n",
    "    # Dictionary to hold edge activity data\n",
    "    edge_data_dict = {}\n",
    "\n",
    "    for date in survey_dates:\n",
    "        G = calculate_network(date, nodes_data_bool, confluence_nodes, stretches, trib_key)\n",
    "\n",
    "        for edge in G.edges:\n",
    "            stretch_status = G.edges[edge].get(\"stretch_activity\", 'Not available')\n",
    "            edge_key = (edge[0], edge[1])\n",
    "            if edge_key not in edge_data_dict:\n",
    "                edge_data_dict[edge_key] = {}\n",
    "            edge_data_dict[edge_key][f'{date}'] = stretch_status\n",
    "\n",
    "    # Build the stretch activity DataFrame\n",
    "    all_stretch_data = []\n",
    "    for (node_start, node_end), activities in edge_data_dict.items():\n",
    "        edge_info = {'Node Start': node_start, 'Node End': node_end}\n",
    "        edge_info.update(activities)\n",
    "        all_stretch_data.append(edge_info)\n",
    "\n",
    "    stretches_activity_df = pd.DataFrame(all_stretch_data)\n",
    "\n",
    "    # Merge stretch lengths\n",
    "    stretches_data = pd.merge(\n",
    "        stretches_activity_df,\n",
    "        stretches[['Node Start', 'Node End', 'Stretch Length (km)']],\n",
    "        on=['Node Start', 'Node End'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Reorder columns: length after Node End\n",
    "    column_order = ['Node Start', 'Node End', 'Stretch Length (km)'] + \\\n",
    "                   [col for col in stretches_data.columns if col not in ['Node Start', 'Node End', 'Stretch Length (km)']]\n",
    "    stretches_data = stretches_data[column_order]\n",
    "\n",
    "    return stretches_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d93143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold results for each tributary\n",
    "all_stretch_results = {}\n",
    "\n",
    "# Loop over each tributary in the bool_network_data\n",
    "for trib in bool_network_data:\n",
    "    # Get all survey dates for the current tributary\n",
    "    survey_dates = [col for col in bool_network_data[trib]['nodes'].columns if \"202\" in col]\n",
    "    \n",
    "    # Calculate stretch activity DataFrame\n",
    "    stretch_df = collect_stretch_activities(trib, survey_dates, bool_network_data)\n",
    "    \n",
    "    # Store result\n",
    "    all_stretch_results[trib] = stretch_df\n",
    "\n",
    "    print(f\"Processed stretch activity for {trib}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaef25a3",
   "metadata": {},
   "source": [
    "# Computing Persistency Index (P_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c27be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate persistency index (P_i) for each stretch\n",
    "def persistence_index(df):\n",
    "    # Identify columns related to Stretch Activity\n",
    "    activity_columns = [col for col in df.columns if '202' in col]\n",
    "    \n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_numeric = df.copy()\n",
    "    \n",
    "    # Replace activity status with numeric values\n",
    "    pd.set_option('future.no_silent_downcasting', True) #To avoid future warning\n",
    "    \n",
    "    df['Stretch Length (km)'] = pd.to_numeric(df['Stretch Length (km)'], errors='coerce')\n",
    "    df_numeric[activity_columns] = df_numeric[activity_columns].replace({\n",
    "        'Active': 1,\n",
    "        'Not Active': 0,\n",
    "        'Disconnected':0.5,\n",
    "        'Not enough information': np.nan\n",
    "    }).astype(float)\n",
    "    \n",
    "    # Calculate the number of 'Active' states\n",
    "    df['Active Count'] = df_numeric[activity_columns].sum(axis=1)\n",
    "    \n",
    "    # Calculate the total number of valid observations\n",
    "    df['Valid Observations Count'] = df_numeric[activity_columns].notna().sum(axis=1)\n",
    "    \n",
    "    # Compute the proportion of 'Active' states\n",
    "    df['Persistency Index (P_i)'] = df['Active Count'] / df['Valid Observations Count']\n",
    "    \n",
    "    # Drop intermediate columns used for calculation\n",
    "    df.drop(columns=['Active Count', 'Valid Observations Count'], inplace=True)\n",
    "    \n",
    "    # Reorder columns to place Persistency Index next to Stretch Length column\n",
    "    columns = list(df.columns)\n",
    "    stretch_length_index = columns.index('Stretch Length (km)')\n",
    "    # Move 'Persistency Index (P_i)' next to 'Stretch Length (km)'\n",
    "    columns.insert(stretch_length_index + 1, columns.pop(columns.index('Persistency Index (P_i)')))\n",
    "    df = df[columns]\n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5406092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trib, df in all_stretch_results.items():\n",
    "    all_stretch_results[trib] = persistence_index(df)\n",
    "    print(f\"Calculated Persistency Index for {trib}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74acd088",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trib, stretches_data in all_stretch_results.items():\n",
    "    print(f\"--- Results for {trib} ---\")\n",
    "    \n",
    "    # Count stretches with Persistency Index = 1\n",
    "    persistency_1_count = stretches_data[stretches_data['Persistency Index (P_i)'] == 1].shape[0]\n",
    "    print(f\"Number of stretches with persistency = 1: {persistency_1_count}\")\n",
    "    \n",
    "    # Total length of stretches with Persistency Index = 1\n",
    "    total_length_persistency_1 = stretches_data[stretches_data['Persistency Index (P_i)'] == 1]['Stretch Length (km)'].sum()\n",
    "    print(f\"Total length of stretches with persistency = 1: {total_length_persistency_1:.2f} km\")\n",
    "    \n",
    "    # Total length of all stretches\n",
    "    total_length = stretches_data['Stretch Length (km)'].sum()\n",
    "    print(f\"Total length of all stretches: {total_length:.2f} km\")\n",
    "    \n",
    "    # Calculate ephemeral percentage\n",
    "    percentage_ephemeral = ((total_length - total_length_persistency_1) / total_length) * 100\n",
    "    print(f\"Ephemeral percentage: {percentage_ephemeral:.2f}%\")\n",
    "    \n",
    "    # Mean Persistency Index (P_i)\n",
    "    mean_pi = stretches_data['Persistency Index (P_i)'].mean()\n",
    "    print(f\"Mean Persistency Index (P_i): {mean_pi:.3f}\")\n",
    "    \n",
    "    # Mean Persistency Index (P_i) for stretches where P_i != 1\n",
    "    mean_pi_not_1 = stretches_data[stretches_data['Persistency Index (P_i)'] != 1]['Persistency Index (P_i)'].mean()\n",
    "    print(f\"Mean Persistency Index (P_i) for stretches where P_i â‰  1: {mean_pi_not_1:.3f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6bd908",
   "metadata": {},
   "source": [
    "# Computing ADNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc436811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ADNL(df, survey_date):\n",
    "    # Create dictionaries to store results for each date\n",
    "    ADNL = {}\n",
    "    ADNL_p = {}\n",
    "    ADNL_p_o = {}\n",
    "    total_length = {date: 0 for date in survey_date}\n",
    "\n",
    "    # Process each survey date\n",
    "    for date in survey_date:\n",
    "\n",
    "        # Create a directed graph for the current date\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        # Add edges to the graph for the current date\n",
    "        for _, row in df.iterrows():\n",
    "            G.add_edge(row['Node Start'], row['Node End'], \n",
    "                       length=row['Stretch Length (km)'], \n",
    "                       activity=row[date])\n",
    "\n",
    "        # Find all downstream nodes (nodes with no outgoing edges)\n",
    "        downstream_nodes = [node for node in G.nodes if G.out_degree(node) == 0]\n",
    "\n",
    "        def dfs(node, visited):\n",
    "            if node in visited:\n",
    "                return  # Exit if this node has already been processed\n",
    "            visited.add(node)  # Mark the node as visited\n",
    "\n",
    "            incoming_edges = list(G.in_edges(node))\n",
    "            for u, v in incoming_edges:\n",
    "                activity = G.edges[u, v].get('activity', '')\n",
    "                stretch_length = G.edges[u, v].get('length', 0)\n",
    "\n",
    "                if activity == 'Active':\n",
    "                    total_length[date] += stretch_length\n",
    "                    dfs(u, visited)\n",
    "\n",
    "        # Start DFS from each downstream node\n",
    "        for node in downstream_nodes:\n",
    "            visited = set()  # Reset visited for each downstream node\n",
    "            dfs(node, visited)\n",
    "\n",
    "        # Calculate the total active length for the current survey date\n",
    "        ADNL[date] = total_length[date]\n",
    "\n",
    "        # Calculate total stretch length from the stretches_data DataFrame\n",
    "        total_stretch_length = stretches_data['Stretch Length (km)'].sum()\n",
    "\n",
    "        # Filter rows where the activity status is 'Active', 'Inactive', or 'Disconnected'\n",
    "        valid_rows = df[date].isin(['Active', 'Not Active', 'Disconnected'])\n",
    "        total_observed_stretch_length = df.loc[valid_rows, 'Stretch Length (km)'].sum()\n",
    "\n",
    "        # Calculate the percentage of ADNL based on the total stretch length\n",
    "        ADNL_p[date] = ADNL[date] / total_stretch_length if total_stretch_length > 0 else np.nan\n",
    "\n",
    "        # Calculate the percentage of ADNL based on the observed stretch length\n",
    "        ADNL_p_o[date] = ADNL[date] / total_observed_stretch_length if total_observed_stretch_length > 0 else np.nan\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    ANDL_df = pd.DataFrame({\n",
    "        'Survey Date': list(ADNL.keys()),\n",
    "        'Active Network Drainage Length': list(ADNL.values()),\n",
    "        'Percentage of Total Network': [ADNL_p[date] for date in ADNL.keys()],\n",
    "        'Percentage of Observed Network': [ADNL_p_o[date] for date in ADNL.keys()]\n",
    "    })\n",
    "\n",
    "    return ANDL_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0710dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNL_results = {}\n",
    "\n",
    "for trib, stretches_data in all_stretch_results.items():\n",
    "    # Calculate ADNL DataFrame for this tributary\n",
    "    ADNL_df = calculate_ADNL(stretches_data, survey_dates)\n",
    "    \n",
    "    # Store result keyed by tributary\n",
    "    ADNL_results[trib] = ADNL_df\n",
    "    \n",
    "ADNL_combined = []\n",
    "\n",
    "for trib, adnl_df in ADNL_results.items():\n",
    "    # Filter rows with no missing information\n",
    "    filtered_df = adnl_df[adnl_df['Percentage of Observed Network'] == adnl_df['Percentage of Total Network']].copy()\n",
    "    \n",
    "    # Add a tributary column to keep track of source\n",
    "    filtered_df['Tributary'] = trib\n",
    "    \n",
    "    ADNL_combined.append(filtered_df)\n",
    "\n",
    "# Concatenate all filtered ADNL dataframes\n",
    "ADNL_df = pd.concat(ADNL_combined, ignore_index=True)\n",
    "ADNL_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2143d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Drainage_ID' and calculate the max and min total length for each group\n",
    "drainage_length_stats = ADNL_df.groupby('Tributary')['Percentage of Total Network'].agg(['max', 'min', 'mean', 'median', 'std'])\n",
    "drainage_length_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7976c7aa",
   "metadata": {},
   "source": [
    "# Z-score analysis for each tributary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8768f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706e7f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"SummaryStatistics.xlsx\", sheet_name=\"Sheet1\", header=0)\n",
    "columns = list(df.columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d232b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify metrics (excluding Catchment ID)\n",
    "metrics = [col for col in df.columns if col != 'Catchment ID']\n",
    "\n",
    "# Separate LCRW row\n",
    "lcrw = df[df['Catchment ID'] == 'LCRW'].iloc[0]\n",
    "\n",
    "# Subset only tributaries\n",
    "tribs = df[df['Catchment ID'].str.contains('Tributary')].copy()\n",
    "\n",
    "# Compute mean and std for each metric across tributaries\n",
    "trib_mean = tribs[metrics].mean()\n",
    "trib_std = tribs[metrics].std()\n",
    "\n",
    "# Calculate z-scores for each tributary relative to LCRW baseline using LCRW mean/std\n",
    "for metric in metrics:\n",
    "    # z = (tributary_value - LCRW_value) / std_of_tributaries\n",
    "    tribs[f'z_LCRW_{metric}'] = (tribs[metric] - lcrw[metric]) / trib_std[metric] # How different is each tributary from the overall watershed?\n",
    "    \n",
    "# Calculate z-scores for each tributary relative to other tributaries using trib mean/std\n",
    "for metric in metrics:\n",
    "    # z = (tributary_value - LCRW_value) / std_of_tributaries\n",
    "    tribs[f'z_trib_{metric}'] = (tribs[metric] - tribs[metric].mean()) / trib_std[metric] # How different is each tributary from the others?\n",
    "\n",
    "tribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textwrap\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# -------- Set up the metric labels --------\n",
    "metrics = [col for col in df.columns if not col.startswith('z') and col != 'Catchment ID']\n",
    "wrapped_metrics = ['\\n'.join(textwrap.wrap(m, width=14)) for m in metrics]\n",
    "\n",
    "# -------- Define color codes --------\n",
    "color_codes = {\n",
    "    'both': \"#3d3e3c\",       # dark gray\n",
    "    'lcrw': \"#B4B4B2\",       # light gray\n",
    "    'tribs': \"#616162\",      # medium gray\n",
    "    'none': 'white'          # no significance\n",
    "}\n",
    "\n",
    "# -------- Assign colors --------\n",
    "def assign_color(row):\n",
    "    colors = []\n",
    "    for m in metrics:\n",
    "        from_lcrw = abs(row[f'z_LCRW_{m}']) > 1\n",
    "        from_tribs = abs(row[f'z_trib_{m}']) > 1\n",
    "\n",
    "        if from_lcrw and from_tribs:\n",
    "            colors.append(color_codes['both'])\n",
    "        elif from_lcrw:\n",
    "            colors.append(color_codes['lcrw'])\n",
    "        elif from_tribs:\n",
    "            colors.append(color_codes['tribs'])\n",
    "        else:\n",
    "            colors.append(color_codes['none'])\n",
    "    return colors\n",
    "\n",
    "# Create color matrix for plotting\n",
    "color_matrix = tribs.apply(assign_color, axis=1, result_type='expand')\n",
    "color_matrix.columns = metrics\n",
    "color_matrix.index = tribs['Catchment ID']\n",
    "\n",
    "# -------- Plot --------\n",
    "fig, ax = plt.subplots(figsize=(len(metrics)*1.2, len(color_matrix)*1.2))\n",
    "\n",
    "# Plot base grid\n",
    "sns.heatmap(\n",
    "    np.ones_like(color_matrix.values, dtype=float),\n",
    "    cmap=[\"white\"],\n",
    "    cbar=False,\n",
    "    linewidths=1,\n",
    "    linecolor='gray',\n",
    "    xticklabels=wrapped_metrics,\n",
    "    yticklabels=color_matrix.index,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Overlay colors\n",
    "for y in range(color_matrix.shape[0]):\n",
    "    for x in range(color_matrix.shape[1]):\n",
    "        rect = plt.Rectangle((x, y), 1, 1,\n",
    "                             facecolor=color_matrix.iloc[y, x],\n",
    "                             edgecolor='gray')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "# Add custom legend\n",
    "legend_elements = [\n",
    "    Patch(facecolor=color_codes['both'], edgecolor='gray', label='Different from both'),\n",
    "    Patch(facecolor=color_codes['lcrw'], edgecolor='gray', label='Different from LCRW'),\n",
    "    Patch(facecolor=color_codes['tribs'], edgecolor='gray', label='Different from other tributaries'),\n",
    "    Patch(facecolor=color_codes['none'], edgecolor='gray', label='Not significantly different')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1.18, 1), fontsize=9)\n",
    "\n",
    "# Clean up ticks\n",
    "ax.tick_params(axis='x', labelrotation=0, labelsize=10)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c8f3ff",
   "metadata": {},
   "source": [
    "# Spatial Analysis: relationships between ephemeral extent and... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d6f36c",
   "metadata": {},
   "source": [
    "## ...land cover, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c443a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_copy = df.copy()\n",
    "df_copy = df_copy.set_index('Catchment ID')\n",
    "df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5228304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of stats columns\n",
    "stats_cols = ['Average Pi ephemeral',\n",
    " 'Ephemeral Extent',\n",
    " 'Relative ADNL-max',\n",
    " 'Relative ADNL-min',\n",
    " 'Relative ADNL-range',\n",
    " 'Relative ADNL-mean',\n",
    " 'Relative ADNL-median',\n",
    " 'Relative ADNL-std' ]\n",
    "\n",
    "# Extract land cover unit columns (everything except the stats)\n",
    "landcover_unit_cols = [col for col in df_copy.columns if col not in stats_cols]\n",
    "\n",
    "# Columns to analyze correlation for: geo units + stats\n",
    "columns_to_analyze = landcover_unit_cols + stats_cols\n",
    "\n",
    "# Calculate correlation matrix for selected columns\n",
    "corr_matrix = df_copy[columns_to_analyze].corr()\n",
    "\n",
    "# We want correlation of geo units (rows) against stats (columns)\n",
    "corr_landcover_vs_stats = corr_matrix.loc[landcover_unit_cols, stats_cols]\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    corr_landcover_vs_stats,\n",
    "    annot=True,\n",
    "    cmap='coolwarm_r',   # Blue = positive, red = negative correlations\n",
    "    center=0,\n",
    "    fmt=\".2f\",\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={'label': 'Correlation Coefficient'}\n",
    ")\n",
    "plt.title(\"Correlation between Geo Unit Percentages and ADNL Stats\")\n",
    "plt.xlabel(\"ADNL Statistics\")\n",
    "plt.ylabel(\"Land Cover\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b92b5a6",
   "metadata": {},
   "source": [
    "## soil type, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe69c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "soil_type = pickle.load(open(\"soil_type_distribution.pkl\", 'rb'))\n",
    "soil_type_T = soil_type.T\n",
    "soil_type_T = soil_type_T.rename(index={\n",
    "    'IBP': 'Tributary A',\n",
    "    'CalUnion': 'Tributary B',\n",
    "    'CherryCreek': 'Tributary C'\n",
    "})\n",
    "\n",
    "soil_type_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce60c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge df\n",
    "combined_soil_df = df_copy.join(soil_type_T, how='left')\n",
    "combined_soil_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a046f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Extract geo unit columns (everything except the stats)\n",
    "soil_unit_cols = [col for col in combined_soil_df.columns if col not in stats_cols]\n",
    "\n",
    "# Initialize matrices\n",
    "corr_values = pd.DataFrame(index=soil_unit_cols, columns=stats_cols)\n",
    "p_values = pd.DataFrame(index=soil_unit_cols, columns=stats_cols)\n",
    "\n",
    "for soil_col in soil_unit_cols:\n",
    "    for stat_col in stats_cols:\n",
    "        soil_data = combined_soil_df[soil_col]\n",
    "        stat_data = combined_soil_df[stat_col]\n",
    "        \n",
    "        # Skip if either is constant or has NaNs\n",
    "        if soil_data.nunique() <= 1 or stat_data.nunique() <= 1:\n",
    "            continue\n",
    "        \n",
    "        r, p = pearsonr(soil_data, stat_data)\n",
    "        corr_values.loc[soil_col, stat_col] = r\n",
    "        p_values.loc[soil_col, stat_col] = p\n",
    "\n",
    "# Convert to numeric (optional cleanup step)\n",
    "corr_values = corr_values.astype(float)\n",
    "p_values = p_values.astype(float)\n",
    "\n",
    "# Mask for statistical significance (e.g., p < 0.05)\n",
    "significance_mask = p_values < 0.05\n",
    "\n",
    "# # Plot only statistically significant correlations\n",
    "# Step 1: Keep only rows and columns with at least one significant p-value\n",
    "rows_to_keep = significance_mask.any(axis=1)\n",
    "cols_to_keep = significance_mask.any(axis=0)\n",
    "\n",
    "# Step 2: Filter all related dataframes\n",
    "corr_values_sig = corr_values.loc[rows_to_keep, cols_to_keep]\n",
    "p_values_sig = p_values.loc[rows_to_keep, cols_to_keep]\n",
    "significance_mask_sig = significance_mask.loc[rows_to_keep, cols_to_keep]\n",
    "\n",
    "# Step 3: Plot the filtered matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    corr_values_sig,\n",
    "    annot=True,\n",
    "    mask=~significance_mask_sig,\n",
    "    cmap='coolwarm_r',\n",
    "    center=0,\n",
    "    fmt=\".2f\",\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={'label': 'Correlation Coefficient'}\n",
    ")\n",
    "plt.title(\"Significant Correlations between Soil Units and ADNL Statistics (p < 0.05)\")\n",
    "plt.xlabel(\"ADNL Statistics\")\n",
    "plt.ylabel(\"Soil Unit Types\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804ecc53",
   "metadata": {},
   "source": [
    "## and surficial geology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e33ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "geo_type = pickle.load(open(\"geological_distribution.pkl\", 'rb'))\n",
    "geo_type_T = geo_type.T\n",
    "geo_type_T = geo_type_T.rename(index={\n",
    "    'IBP': 'Tributary A',\n",
    "    'CalUnion': 'Tributary B',\n",
    "    'CherryCreek': 'Tributary C'\n",
    "})\n",
    "\n",
    "geo_type_T\n",
    "#Merge df\n",
    "combined_geo_df = df_copy.join(geo_type_T, how='left')\n",
    "combined_geo_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf88da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Extract geo unit columns (everything except the stats)\n",
    "geo_unit_cols = [col for col in combined_geo_df.columns if col not in stats_cols]\n",
    "\n",
    "# Initialize matrices\n",
    "corr_values = pd.DataFrame(index=geo_unit_cols, columns=stats_cols)\n",
    "p_values = pd.DataFrame(index=geo_unit_cols, columns=stats_cols)\n",
    "\n",
    "for geo_col in geo_unit_cols:\n",
    "    for stat_col in stats_cols:\n",
    "        geo_data = combined_geo_df[geo_col]\n",
    "        stat_data = combined_geo_df[stat_col]\n",
    "        \n",
    "        # Skip if either is constant or has NaNs\n",
    "        if geo_data.nunique() <= 1 or stat_data.nunique() <= 1:\n",
    "            continue\n",
    "        \n",
    "        r, p = pearsonr(geo_data, stat_data)\n",
    "        corr_values.loc[geo_col, stat_col] = r\n",
    "        p_values.loc[geo_col, stat_col] = p\n",
    "\n",
    "# Convert to numeric (optional cleanup step)\n",
    "corr_values = corr_values.astype(float)\n",
    "p_values = p_values.astype(float)\n",
    "\n",
    "# Mask for statistical significance (e.g., p < 0.05)\n",
    "significance_mask = p_values < 0.05\n",
    "\n",
    "# # Plot only statistically significant correlations\n",
    "# Step 1: Keep only rows and columns with at least one significant p-value\n",
    "rows_to_keep = significance_mask.any(axis=1)\n",
    "cols_to_keep = significance_mask.any(axis=0)\n",
    "\n",
    "# Step 2: Filter all related dataframes\n",
    "corr_values_sig = corr_values.loc[rows_to_keep, cols_to_keep]\n",
    "p_values_sig = p_values.loc[rows_to_keep, cols_to_keep]\n",
    "significance_mask_sig = significance_mask.loc[rows_to_keep, cols_to_keep]\n",
    "\n",
    "# Step 3: Plot the filtered matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    corr_values_sig,\n",
    "    annot=True,\n",
    "    mask=~significance_mask_sig,\n",
    "    cmap='coolwarm_r',\n",
    "    center=0,\n",
    "    fmt=\".2f\",\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={'label': 'Correlation Coefficient'}\n",
    ")\n",
    "plt.title(\"Significant Correlations between Soil Units and ADNL Statistics (p < 0.05)\")\n",
    "plt.xlabel(\"ADNL Statistics\")\n",
    "plt.ylabel(\"Soil Unit Types\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_FC (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
