{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d64798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b105b",
   "metadata": {},
   "source": [
    "# Converting Node Observations into Network Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc19a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = Path(\"..\") / \"Data\" / \"SurveyData.xlsx\"\n",
    "\n",
    "# Load data from Excel files\n",
    "nodes_data = pd.read_excel(PATH_TO_DATA, sheet_name=\"LCRW_Survey_Data\", header=0)\n",
    "# nodes_data = pd.read_excel(PATH_TO_DATA, sheet_name=\"Data_LCRW_Modified_2025_02_17\", header=1) #To include more surveys that before were 'locaiton not accessible' 2025_02_17\n",
    "confluence_nodes = pd.read_excel(PATH_TO_DATA, sheet_name=\"LCRW_Ghost_Nodes\")\n",
    "stretches = pd.read_excel(PATH_TO_DATA, sheet_name=\"LCRW_Edges\").astype(str)  # Convert all values to strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef94493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with boolean values\n",
    "nodes_data_bool = nodes_data.copy()  # Copy the original DataFrame to avoid modifying it\n",
    "\n",
    "#Apply to each column\n",
    "survey_date = [col for col in nodes_data.columns if \"202\" in col]\n",
    "\n",
    "for col in survey_date:\n",
    "    nodes_data_bool[col] = nodes_data[col].map({\n",
    "        \"Yes\": True,\n",
    "        \"No\": False\n",
    "    })\n",
    "\n",
    "# nodes_data_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786a2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_network(survey_date, nodes_data_bool, confluence_nodes, stretches):\n",
    "    # Ensure the None values are correctly registered\n",
    "    nodes_data_bool = nodes_data_bool.where(nodes_data_bool.notnull(), None)\n",
    "\n",
    "    # Create directed network\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Create nodes and add the attribute corresponding to data\n",
    "    for _, data in nodes_data_bool.iterrows():\n",
    "        node_ID = str(data[\"Point ID\"])\n",
    "        G.add_node(node_ID, pos=(data['Longitude'], data['Latitude']), node_activity=data[survey_date])\n",
    "\n",
    "    # Add confluence nodes\n",
    "    for _, row in confluence_nodes.iterrows():\n",
    "        G.add_node(row['Node'], pos=(row['Longitude'], row['Latitude']), node_activity=None)  # Set a default value\n",
    "\n",
    "    # Explicitly set activity for a necessary nodes (e.g., those that correspond to Little Calumet River)\n",
    "    g9_node = \"g9\"  \n",
    "    G.nodes[g9_node][\"node_activity\"] = True\n",
    "    \n",
    "    g0_node = \"g0\"  \n",
    "    G.nodes[g0_node][\"node_activity\"] = True\n",
    "    \n",
    "    # Add network edges // river stretches\n",
    "    for _, row in stretches.iterrows():\n",
    "        if row[\"Node Start\"] in G.nodes and row[\"Node End\"] in G.nodes:\n",
    "            G.add_edge(row[\"Node Start\"], row[\"Node End\"])\n",
    "\n",
    "    # Definitions to calculate the value of each node\n",
    "    def find_upstream(node_ID):\n",
    "        if G.nodes[node_ID].get(\"node_activity\", None) is not None:\n",
    "            return G.nodes[node_ID][\"node_activity\"]\n",
    "        \n",
    "        is_upstream_wet = None\n",
    "        for up_index, _ in G.in_edges(node_ID):\n",
    "            res = find_upstream(up_index)\n",
    "            if res is not None:\n",
    "                is_upstream_wet = is_upstream_wet or res\n",
    "        return is_upstream_wet\n",
    "        \n",
    "    def find_downstream(node_ID):\n",
    "        if G.nodes[node_ID].get(\"node_activity\", None) is not None:\n",
    "            return G.nodes[node_ID][\"node_activity\"]\n",
    "        \n",
    "        is_downstream_wet = None\n",
    "        for _, down_index in G.out_edges(node_ID):\n",
    "            res = find_downstream(down_index)\n",
    "            if res is not None:\n",
    "                is_downstream_wet = is_downstream_wet or res\n",
    "        return is_downstream_wet\n",
    "\n",
    "    # First pass: Initial assignment of node activity\n",
    "    for node_ID in G.nodes:\n",
    "        upstream = find_upstream(node_ID)\n",
    "        downstream = find_downstream(node_ID)\n",
    "        if upstream == downstream:\n",
    "            G.nodes[node_ID][\"node_activity\"] = upstream\n",
    "            \n",
    "    # # Check that all nodes have been assigned\n",
    "    # for node_ID in G.nodes:\n",
    "    #     node_attrs = G.nodes[node_ID]\n",
    "    #     if node_attrs.get(\"node_activity\", None) is None:\n",
    "    #         print(node_ID)\n",
    "\n",
    "    # Second pass: Assign \"disconnected\" or \"indeterminate\"\n",
    "    for node_ID in G.nodes:\n",
    "        if G.nodes[node_ID][\"node_activity\"] is None:\n",
    "            upstream = find_upstream(node_ID)\n",
    "            downstream = find_downstream(node_ID)\n",
    "\n",
    "            if upstream is not None and downstream is not None:\n",
    "                if upstream != downstream:\n",
    "                    G.nodes[node_ID][\"node_activity\"] = \"disconnected\"\n",
    "            else:\n",
    "                G.nodes[node_ID][\"node_activity\"] = None\n",
    "\n",
    "\n",
    "    # Assign stretch attribute based on node activity\n",
    "    for up_index, down_index in G.edges:\n",
    "        up_activity = G.nodes[up_index].get(\"node_activity\")\n",
    "        down_activity = G.nodes[down_index].get(\"node_activity\")\n",
    "        \n",
    "        if up_activity is True and down_activity is True:\n",
    "            G.edges[up_index, down_index][\"color\"] = 'blue'\n",
    "        elif up_activity is False and down_activity is False:\n",
    "            G.edges[up_index, down_index][\"color\"] = 'red'\n",
    "        elif up_activity is True and down_activity is False:\n",
    "            G.edges[up_index, down_index][\"color\"] = 'orange'\n",
    "        elif up_activity is False and down_activity is True:\n",
    "            G.edges[up_index, down_index][\"color\"] = 'orange'\n",
    "        elif \"disconnected\" in (up_activity, down_activity):\n",
    "            G.edges[up_index, down_index][\"color\"] = 'orange'\n",
    "        else:\n",
    "            G.edges[up_index, down_index][\"color\"] = 'grey'\n",
    "\n",
    "    # Assign activity to stretches\n",
    "    for up_index, down_index in G.edges:\n",
    "        up_activity = G.nodes[up_index].get(\"node_activity\")\n",
    "        down_activity = G.nodes[down_index].get(\"node_activity\")\n",
    "        \n",
    "        if up_activity is True and down_activity is True:\n",
    "            G.edges[up_index, down_index][\"stretch_activity\"] = 'Active'\n",
    "        elif up_activity is False and down_activity is False:\n",
    "            G.edges[up_index, down_index][\"stretch_activity\"] = 'Not Active'\n",
    "        elif up_activity is True and down_activity is False:\n",
    "            G.edges[up_index, down_index][\"stretch_activity\"] = 'Disconnected'\n",
    "        elif up_activity is False and down_activity is True:\n",
    "            G.edges[up_index, down_index][\"stretch_activity\"] = 'Disconnected'\n",
    "        elif \"disconnected\" in (up_activity, down_activity):\n",
    "            G.edges[up_index, down_index][\"stretch_activity\"] = 'Disconnected'\n",
    "        else:\n",
    "            G.edges[up_index, down_index][\"stretch_activity\"] = 'Not enough information'\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba9437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to collect stretch activities into a DataFrame\n",
    "def collect_stretch_activities(survey_dates, nodes_data_bool, confluence_nodes, stretches):\n",
    "    # Dictionary to hold data for DataFrame creation\n",
    "    edge_data_dict = {}\n",
    "\n",
    "    for date in survey_dates:\n",
    "        G = calculate_network(date, nodes_data_bool, confluence_nodes, stretches)\n",
    "        \n",
    "        # Collect data for the current survey date\n",
    "        for edge in G.edges:\n",
    "            stretch_status = G.edges[edge].get(\"stretch_activity\", 'Not available')\n",
    "            # Create a unique key for each edge\n",
    "            edge_key = (edge[0], edge[1])\n",
    "            if edge_key not in edge_data_dict:\n",
    "                edge_data_dict[edge_key] = {}\n",
    "            edge_data_dict[edge_key][f'{date}'] = stretch_status\n",
    "\n",
    "    # Convert the dictionary into a DataFrame\n",
    "    # Prepare a list of dictionaries to create the DataFrame\n",
    "    all_stretch_data = []\n",
    "    for (node_start, node_end), activities in edge_data_dict.items():\n",
    "        # Merge the activities into a single dictionary with Node Start and Node End\n",
    "        edge_info = {'Node Start': node_start, 'Node End': node_end}\n",
    "        edge_info.update(activities)\n",
    "        all_stretch_data.append(edge_info)\n",
    "\n",
    "    # Create DataFrame from the list of dictionaries\n",
    "    stretches_activity_df = pd.DataFrame(all_stretch_data)\n",
    "    \n",
    "   # Merge the stretch lengths into the DataFrame\n",
    "    stretches_data = pd.merge(stretches_activity_df, stretches[['Node Start', 'Node End', 'Stretch Length (km)']], \n",
    "                                        on=['Node Start', 'Node End'], how='left')\n",
    "    \n",
    "    # Reorder columns to have \"length\" after \"Node End\"\n",
    "    column_order = ['Node Start', 'Node End', 'Stretch Length (km)'] + [col for col in stretches_data.columns if col not in ['Node Start', 'Node End', 'Stretch Length (km)']]\n",
    "    stretches_data = stretches_data[column_order]\n",
    "    \n",
    "    return stretches_data\n",
    "\n",
    "# Collect stretch activities and create DataFrame\n",
    "stretches_data = collect_stretch_activities(survey_date, nodes_data_bool, confluence_nodes, stretches)\n",
    "\n",
    "#Print the full DataFrame // Set display options t5o see full df\n",
    "pd.set_option('display.max_rows', None)  # None means unlimited rows\n",
    "pd.set_option('display.max_columns', None)  # None means unlimited columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e8831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data as a pickle file\n",
    "stretches_data.to_csv(\"LCRW_stretches_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaef25a3",
   "metadata": {},
   "source": [
    "# Computing Persistency Index (P_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c27be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate persistency index (P_i) for each stretch\n",
    "def persistence_index(df):\n",
    "    # Identify columns related to Stretch Activity\n",
    "    activity_columns = [col for col in df.columns if '202' in col]\n",
    "    \n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_numeric = df.copy()\n",
    "    \n",
    "    # Replace activity status with numeric values\n",
    "    pd.set_option('future.no_silent_downcasting', True) #To avoid future warning\n",
    "    \n",
    "    df['Stretch Length (km)'] = pd.to_numeric(df['Stretch Length (km)'], errors='coerce')\n",
    "    df_numeric[activity_columns] = df_numeric[activity_columns].replace({\n",
    "        'Active': 1,\n",
    "        'Not Active': 0,\n",
    "        'Disconnected':0.5,\n",
    "        'Not enough information': np.nan\n",
    "    }).astype(float)\n",
    "    \n",
    "    # Calculate the number of 'Active' states\n",
    "    df['Active Count'] = df_numeric[activity_columns].sum(axis=1)\n",
    "    \n",
    "    # Calculate the total number of valid observations\n",
    "    df['Valid Observations Count'] = df_numeric[activity_columns].notna().sum(axis=1)\n",
    "    \n",
    "    # Compute the proportion of 'Active' states\n",
    "    df['Persistency Index (P_i)'] = df['Active Count'] / df['Valid Observations Count']\n",
    "    \n",
    "    # Drop intermediate columns used for calculation\n",
    "    df.drop(columns=['Active Count', 'Valid Observations Count'], inplace=True)\n",
    "    \n",
    "    # Reorder columns to place Persistency Index next to Stretch Length column\n",
    "    columns = list(df.columns)\n",
    "    stretch_length_index = columns.index('Stretch Length (km)')\n",
    "    # Move 'Persistency Index (P_i)' next to 'Stretch Length (km)'\n",
    "    columns.insert(stretch_length_index + 1, columns.pop(columns.index('Persistency Index (P_i)')))\n",
    "    df = df[columns]\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "# Add the activity proportion column to the existing DataFrame\n",
    "stretches_data = persistence_index(stretches_data)\n",
    "\n",
    "# #Print the full DataFrame // Set display options t5o see full df\n",
    "# pd.set_option('display.max_rows', None)  # None means unlimited rows\n",
    "# pd.set_option('display.max_columns', None)  # None means unlimited columns\n",
    "\n",
    "# stretches_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74acd088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many stretches have Persistency Index (P_i) = 1\n",
    "persistency_1_count = stretches_data[stretches_data['Persistency Index (P_i)'] == 1].shape[0]\n",
    "print(f\"Number of stretches with persistency = 1: {persistency_1_count}\")\n",
    "\n",
    "# Calculate the total length of stretches with Persistency Index (P_i) = 1\n",
    "total_length_persistency_1 = stretches_data[stretches_data['Persistency Index (P_i)'] == 1]['Stretch Length (km)'].sum()\n",
    "print(f\"Total length of stretches with persistency = 1: {total_length_persistency_1:.2f} km\")\n",
    "\n",
    "# Total length of all stretches\n",
    "total_length = stretches_data['Stretch Length (km)'].sum()\n",
    "print(f\"Total length: {total_length:.2f} km\")\n",
    "\n",
    "# Calculate ephemeral percentage\n",
    "percentage_ephemeral = ((total_length - total_length_persistency_1) / total_length) * 100\n",
    "print(f\"Ephemeral percentage is: {percentage_ephemeral:.2f}%\")\n",
    "\n",
    "# Mean Persistency Index\n",
    "mean_pi = stretches_data['Persistency Index (P_i)'].mean()\n",
    "print(f\"Mean Persistency Index (P_i): {mean_pi:.3f}\")\n",
    "\n",
    "# Mean P_i where P_i ≠ 1\n",
    "mean_pi_not_1 = stretches_data[stretches_data['Persistency Index (P_i)'] != 1]['Persistency Index (P_i)'].mean()\n",
    "print(f\"Mean Persistency Index (P_i) for stretches where P_i ≠ 1: {mean_pi_not_1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7676f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all geospatial data\n",
    "catchment_outline_gdf = gpd.read_file(Path(\"..\") / \"Data\" / \"CatchmentBoundary\" / \"WestLittleCal_Delineation.shp\")\n",
    "\n",
    "tributaries = gpd.read_file(Path(\"..\") / \"Data\" / \"TributaryBoundaries\" / \"tributaries.shp\")\n",
    "tributaries = tributaries.to_crs(epsg=3857)\n",
    "tributaries[\"ID\"] =[\"LCRW\", \"IBP\", \"CalUnion\", \"CherryCreek\"]\n",
    "\n",
    "land_use = gpd.read_file(Path(\"..\") / \"Data\" / \"LandUse\" / \"Catchment_LandUse.shp\")\n",
    "\n",
    "land_cover = Path(\"..\") / \"Data\" / \"LandCover\" / \"Catchment_LandCover.tif\"\n",
    "\n",
    "river_network = gpd.read_file(Path(\"..\") / \"Data\" / \"RiverNetwork\" / \"Catchment_RiverNetwork.shp\")\n",
    "# Filter for the specific FID\n",
    "river_255 = river_network.loc[[255]]  # Double brackets to keep it a GeoDataFrame\n",
    "# Ensure it's in the correct CRS\n",
    "river_255 = river_255.to_crs(epsg=3857)\n",
    "# Clip river network to the extent of the land use data\n",
    "river_fid_255 = gpd.clip(river_255, land_use)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a24e50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject_raster_to_3857(raster_path):\n",
    "    import rasterio\n",
    "    from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        dst_crs = 'EPSG:3857'\n",
    "        transform, width, height = calculate_default_transform(\n",
    "            src.crs, dst_crs, src.width, src.height, *src.bounds\n",
    "        )\n",
    "        kwargs = src.meta.copy()\n",
    "        kwargs.update({\n",
    "            'crs': dst_crs,\n",
    "            'transform': transform,\n",
    "            'width': width,\n",
    "            'height': height\n",
    "        })\n",
    "\n",
    "        destination = np.empty((src.count, height, width), dtype=src.dtypes[0])\n",
    "\n",
    "        for i in range(1, src.count + 1):\n",
    "            reproject(\n",
    "                source=rasterio.band(src, i),\n",
    "                destination=destination[i - 1],\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=transform,\n",
    "                dst_crs=dst_crs,\n",
    "                resampling=Resampling.nearest\n",
    "            )\n",
    "\n",
    "        return destination, transform, kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdc4933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_binary_pi_map(\n",
    "    nodes_data_bool, confluence_nodes, stretches,\n",
    "    catchment_outline_gdf, river_network, tributaries,\n",
    "    land_cover, land_use\n",
    "):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as mcolors\n",
    "    from matplotlib.collections import LineCollection\n",
    "    import matplotlib.patheffects as pe\n",
    "    import geopandas as gpd\n",
    "    import networkx as nx\n",
    "    from shapely.geometry import LineString\n",
    "    import numpy as np\n",
    "\n",
    "    # Create directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes\n",
    "    for _, data in nodes_data_bool.iterrows():\n",
    "        node_ID = str(data[\"Point ID\"])\n",
    "        G.add_node(node_ID, pos=(data['Longitude'], data['Latitude']))\n",
    "    for _, row in confluence_nodes.iterrows():\n",
    "        G.add_node(row['Node'], pos=(row['Longitude'], row['Latitude']))\n",
    "\n",
    "    # Add edges with P_i\n",
    "    for _, row in stretches.iterrows():\n",
    "        if row[\"Node Start\"] in G.nodes and row[\"Node End\"] in G.nodes:\n",
    "            G.add_edge(row[\"Node Start\"], row[\"Node End\"], P_i=row[\"Persistency Index (P_i)\"])\n",
    "\n",
    "    \n",
    "    # Positions and attributes\n",
    "    positions = nx.get_node_attributes(G, \"pos\")\n",
    "    edges = list(G.edges())\n",
    "    pi_values = nx.get_edge_attributes(G, \"P_i\")\n",
    "\n",
    "    # Binary color map: orange if P_i < 1, blue if P_i == 1\n",
    "    edge_colors = [\"#fff828ff\" if pi_values.get(edge, 0) < 1 else '#0085b2' for edge in edges]\n",
    "    edge_lines = [LineString([positions[u], positions[v]]) for u, v in edges]\n",
    "    edge_gdf = gpd.GeoDataFrame(geometry=edge_lines, crs=\"EPSG:4326\")\n",
    "\n",
    "    # Reproject data\n",
    "    edge_gdf = edge_gdf.to_crs(epsg=3857)\n",
    "    catchment_outline_gdf = catchment_outline_gdf.to_crs(epsg=3857)\n",
    "    river_network = river_network.to_crs(epsg=3857)\n",
    "    tributaries = tributaries.to_crs(epsg=3857)\n",
    "    land_use = land_use.to_crs(epsg=3857)\n",
    "\n",
    "    # Clip and prepare background\n",
    "    land_cover_data, land_cover_transform, _ = reproject_raster_to_3857(land_cover)\n",
    "    custom_colors = [\"#efefefee\", \"#c9c9c9\", \"#9e9d9d\"]\n",
    "    custom_cmap = LinearSegmentedColormap.from_list(\"impervious_cmap\", custom_colors, N=256)\n",
    "    extent = [\n",
    "        land_cover_transform[2],\n",
    "        land_cover_transform[2] + land_cover_transform[0] * land_cover_data.shape[2],\n",
    "        land_cover_transform[5] + land_cover_transform[4] * land_cover_data.shape[1],\n",
    "        land_cover_transform[5]\n",
    "    ]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    ax.imshow(\n",
    "        land_cover_data[0],\n",
    "        cmap=custom_cmap,\n",
    "        extent=extent,\n",
    "        interpolation='none',\n",
    "        origin='upper',\n",
    "        zorder=0\n",
    "    )\n",
    "\n",
    "    # Zoom margins\n",
    "    pixel_width = abs(land_cover_transform.a)\n",
    "    pixel_height = abs(land_cover_transform.e)\n",
    "    ax.set_xlim(extent[0] + 40 * pixel_width, extent[1] - 40 * pixel_width)\n",
    "    ax.set_ylim(extent[2] + 10 * pixel_height, extent[3] - 10 * pixel_height)\n",
    "\n",
    "    # Plot data layers\n",
    "    river_network_outside = gpd.overlay(river_network, catchment_outline_gdf, how='difference')\n",
    "    river_network_outside.plot(ax=ax, color='#1c3652', linewidth=1, zorder=1)\n",
    "    river_fid_255.plot(ax=ax, color='#1c3652', linewidth=1)\n",
    "    tributaries.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=1.5, linestyle='--', zorder=4)\n",
    "    catchment_outline_gdf.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=1.5, zorder=3)\n",
    "\n",
    "    edge_collection = LineCollection(\n",
    "        [list(line.coords) for line in edge_gdf.geometry],\n",
    "        colors=edge_colors,\n",
    "        linewidths=2.5,\n",
    "        path_effects=[pe.withStroke(linewidth=5, foreground=\"#000000\")],\n",
    "        zorder=5\n",
    "    )\n",
    "    ax.add_collection(edge_collection)\n",
    "\n",
    "    # Colorbar for interpretation\n",
    "    import matplotlib.patches as mpatches\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(color=\"#0085b2\", label='P_i = 1 (Persistent)'),\n",
    "        mpatches.Patch(color=\"#f3eb0f\", label='P_i < 1 (Ephemeral)')\n",
    "    ]\n",
    "    # ax.legend(handles=legend_elements, loc='lower right', frameon=True)\n",
    "\n",
    "    # Imperviousness colorbar\n",
    "    cbar_raster = plt.colorbar(\n",
    "        plt.cm.ScalarMappable(cmap=custom_cmap, norm=mcolors.Normalize(vmin=land_cover_data.min(), vmax=land_cover_data.max())),\n",
    "        ax=ax,\n",
    "        orientation='horizontal',\n",
    "        fraction=0.03,\n",
    "        pad=0.05,\n",
    "        shrink=0.7\n",
    "    )\n",
    "    cbar_raster.set_label('Percent Impervious Surface (%)')\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    plt.savefig(\"Ephemeral_Classification.png\", bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binary_pi_map(nodes_data_bool, confluence_nodes, stretches_data, catchment_outline_gdf, river_network, tributaries, land_cover, land_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38886365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import numpy as np\n",
    "\n",
    "# Trim off the brightest part of 'GnBu'\n",
    "base_cmap = plt.get_cmap('GnBu')  # This works in all versions\n",
    "cmap = LinearSegmentedColormap.from_list(\n",
    "    'trimmed_GnBu',\n",
    "    base_cmap(np.linspace(0, 0.8, 256))  # Avoid dark tones\n",
    ")\n",
    "\n",
    "def calculate_network_with_pi(\n",
    "    nodes_data_bool, confluence_nodes, stretches,\n",
    "    catchment_outline_gdf, river_network, tributaries,\n",
    "    land_cover, land_use\n",
    "):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as mcolors\n",
    "    from matplotlib.collections import LineCollection\n",
    "    import matplotlib.patheffects as pe\n",
    "    import geopandas as gpd\n",
    "    import networkx as nx\n",
    "    from shapely.geometry import LineString\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    import numpy as np\n",
    "\n",
    "    # Create directed network graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes\n",
    "    for _, data in nodes_data_bool.iterrows():\n",
    "        node_ID = str(data[\"Point ID\"])\n",
    "        G.add_node(node_ID, pos=(data['Longitude'], data['Latitude']))\n",
    "    for _, row in confluence_nodes.iterrows():\n",
    "        G.add_node(row['Node'], pos=(row['Longitude'], row['Latitude']))\n",
    "\n",
    "    # Add edges with Persistency Index\n",
    "    for _, row in stretches.iterrows():\n",
    "        if row[\"Node Start\"] in G.nodes and row[\"Node End\"] in G.nodes:\n",
    "            G.add_edge(row[\"Node Start\"], row[\"Node End\"], Persistency_Index=row[\"Persistency Index (P_i)\"])\n",
    "\n",
    "    # Extract positions and edge persistency\n",
    "    positions = nx.get_node_attributes(G, \"pos\")\n",
    "    edges = list(G.edges())\n",
    "    edge_persistency_index = nx.get_edge_attributes(G, \"Persistency_Index\")\n",
    "    norm = mcolors.Normalize(vmin=0, vmax=1)\n",
    "    # cmap = plt.get_cmap('GnBu')\n",
    "    edge_colors = [cmap(norm(edge_persistency_index.get(edge, 0))) for edge in edges]\n",
    "    edge_lines = [LineString([positions[u], positions[v]]) for u, v in edges]\n",
    "\n",
    "    edge_gdf = gpd.GeoDataFrame(geometry=edge_lines, crs=\"EPSG:4326\")\n",
    "    node_gdf = gpd.GeoDataFrame(\n",
    "        geometry=gpd.points_from_xy(\n",
    "            [pos[0] for pos in positions.values()],\n",
    "            [pos[1] for pos in positions.values()]\n",
    "        ),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    # Reproject all to EPSG:3857\n",
    "    edge_gdf = edge_gdf.to_crs(epsg=3857)\n",
    "    node_gdf = node_gdf.to_crs(epsg=3857)\n",
    "    catchment_outline_gdf = catchment_outline_gdf.to_crs(epsg=3857)\n",
    "    river_network = river_network.to_crs(epsg=3857)\n",
    "    tributaries = tributaries.to_crs(epsg=3857)\n",
    "    land_use = land_use.to_crs(epsg=3857)\n",
    "\n",
    "    # Clip river network to land use extent\n",
    "    river_network_clipped = gpd.clip(river_network, land_use)\n",
    "\n",
    "    # Remove river parts inside catchment\n",
    "    river_network_outside = gpd.overlay(river_network_clipped, catchment_outline_gdf, how='difference')\n",
    "    \n",
    "\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Plot raster background\n",
    "    land_cover_data, land_cover_transform, land_cover_meta = reproject_raster_to_3857(land_cover)\n",
    "\n",
    "    # custom_colors = ['#6e898a', '#B0B0B0']  # dark greenish -> light gray\n",
    "    custom_colors = [\"#efefefee\", \"#c9c9c9\", \"#9e9d9d\"]\n",
    "    custom_cmap = LinearSegmentedColormap.from_list(\"impervious_cmap\", custom_colors, N=256)\n",
    "\n",
    "    extent = [\n",
    "        land_cover_transform[2],  # xmin\n",
    "        land_cover_transform[2] + land_cover_transform[0] * land_cover_data.shape[2],  # xmax\n",
    "        land_cover_transform[5] + land_cover_transform[4] * land_cover_data.shape[1],  # ymin\n",
    "        land_cover_transform[5]  # ymax\n",
    "    ]\n",
    "\n",
    "    ax.imshow(\n",
    "        land_cover_data[0],  # single band raster\n",
    "        cmap=custom_cmap,\n",
    "        extent=extent,\n",
    "        interpolation='none',\n",
    "        origin='upper',\n",
    "        zorder=0\n",
    "    )\n",
    "\n",
    "    # --- Zoom in by 10 pixels on all sides ---\n",
    "\n",
    "    # Calculate pixel size from affine transform\n",
    "    pixel_width = abs(land_cover_transform.a)\n",
    "    pixel_height = abs(land_cover_transform.e)\n",
    "\n",
    "    margin_x = 40 * pixel_width\n",
    "    margin_y = 10 * pixel_height\n",
    "\n",
    "    # Shrink extent by moving edges inward by 10 pixels worth of distance\n",
    "    ax.set_xlim(extent[0] + margin_x, extent[1] - margin_x)\n",
    "    ax.set_ylim(extent[2] + margin_y, extent[3] - margin_y)\n",
    "\n",
    "    # -----------------------------------------\n",
    "\n",
    "    # Set plot limits to land use bounds (clipping to land use extent)\n",
    "    bounds = land_use.total_bounds  # xmin, ymin, xmax, ymax\n",
    "    margin = 100  # meters, for some padding\n",
    "\n",
    "    # Plot river network outside catchment\n",
    "    river_network_outside.plot(ax=ax, color='#1c3652', linewidth=1, zorder=1)\n",
    "    river_fid_255.plot(ax=ax, color='#1c3652', linewidth=1)\n",
    "\n",
    "    # Plot edges with black outline\n",
    "    edge_collection = LineCollection(\n",
    "        [list(line.coords) for line in edge_gdf.geometry],\n",
    "        colors=edge_colors,\n",
    "        linewidths=2.5,\n",
    "        path_effects=[pe.withStroke(linewidth=5, foreground=\"#000000\")],\n",
    "        zorder=5\n",
    "    )\n",
    "    ax.add_collection(edge_collection)\n",
    "\n",
    "    # Plot tributaries\n",
    "    tributaries.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=1.5, linestyle='--', zorder=4)\n",
    "\n",
    "    # Plot catchment outline\n",
    "    catchment_outline_gdf.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=1.5, zorder=3)\n",
    "\n",
    "    # Persistency index colorbar - vertical right side\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "\n",
    "    cbar_pi = plt.colorbar(sm, ax=ax, orientation='vertical', fraction=0.03, pad=0.01, shrink=0.7)\n",
    "    cbar_pi.set_label('Persistency Index $(P_i)$', rotation=90, labelpad=15)\n",
    "\n",
    "    # Raster colorbar below plot (horizontal)\n",
    "    cbar_raster = plt.colorbar(\n",
    "        plt.cm.ScalarMappable(cmap=custom_cmap, norm=mcolors.Normalize(vmin=land_cover_data.min(), vmax=land_cover_data.max())),\n",
    "        ax=ax,\n",
    "        orientation='horizontal',\n",
    "        fraction=0.03,  # smaller length\n",
    "        pad=0.05,\n",
    "        shrink=0.7       # smaller height\n",
    "    )\n",
    "    cbar_raster.set_label('Percent Impervious Surface (%)')\n",
    "\n",
    "    # ax.set_title('Network Visualization of Persistency Index with Basemap')\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    plt.savefig('Persistency_Index.png', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee00cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_network_with_pi(nodes_data_bool, confluence_nodes, stretches_data, catchment_outline_gdf, river_network, tributaries, land_cover, land_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ff1edb",
   "metadata": {},
   "source": [
    "### Viewing impact of engineering on stretch activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b53be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "stretches_type = pd.read_csv(Path(\"..\") / \"Data\" / \"StretchesData.csv\")\n",
    "stretches_type\n",
    "\n",
    "# Prepare data for ANOVA\n",
    "groups = []\n",
    "for cat in ['Natural', 'Lined']:\n",
    "    values = stretches_type.loc[stretches_type['Channel Type'] == cat, 'Persistency Index (P_i)']\n",
    "    groups.append(values.dropna())\n",
    "\n",
    "# One-way ANOVA\n",
    "f_stat, p_val = stats.f_oneway(*groups)\n",
    "print(f\"ANOVA F-statistic: {f_stat:.3f}, p-value: {p_val:.4f}\")\n",
    "\n",
    "if p_val < 0.05:\n",
    "    print(\"There is a statistically significant difference between group means.\")\n",
    "else:\n",
    "    print(\"No statistically significant difference between group means.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6bd908",
   "metadata": {},
   "source": [
    "# Computing ADNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc436811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ADNL(df, survey_date):\n",
    "    # Create dictionaries to store results for each date\n",
    "    ADNL = {}\n",
    "    ADNL_p = {}\n",
    "    ADNL_p_o = {}\n",
    "    total_length = {date: 0 for date in survey_date}\n",
    "\n",
    "    # Process each survey date\n",
    "    for date in survey_date:\n",
    "\n",
    "        # Create a directed graph for the current date\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        # Add edges to the graph for the current date\n",
    "        for _, row in df.iterrows():\n",
    "            G.add_edge(row['Node Start'], row['Node End'], \n",
    "                       length=row['Stretch Length (km)'], \n",
    "                       activity=row[date])\n",
    "\n",
    "        # Find all downstream nodes (nodes with no outgoing edges)\n",
    "        downstream_nodes = [node for node in G.nodes if G.out_degree(node) == 0]\n",
    "\n",
    "        def dfs(node, visited):\n",
    "            if node in visited:\n",
    "                return  # Exit if this node has already been processed\n",
    "            visited.add(node)  # Mark the node as visited\n",
    "\n",
    "            incoming_edges = list(G.in_edges(node))\n",
    "            for u, v in incoming_edges:\n",
    "                activity = G.edges[u, v].get('activity', '')\n",
    "                stretch_length = G.edges[u, v].get('length', 0)\n",
    "\n",
    "                if activity == 'Active':\n",
    "                    total_length[date] += stretch_length\n",
    "                    dfs(u, visited)\n",
    "\n",
    "        # Start DFS from each downstream node\n",
    "        for node in downstream_nodes:\n",
    "            visited = set()  # Reset visited for each downstream node\n",
    "            dfs(node, visited)\n",
    "\n",
    "        # Calculate the total active length for the current survey date\n",
    "        ADNL[date] = total_length[date]\n",
    "\n",
    "        # Calculate total stretch length from the stretches_data DataFrame\n",
    "        total_stretch_length = stretches_data['Stretch Length (km)'].sum()\n",
    "\n",
    "        # Filter rows where the activity status is 'Active', 'Inactive', or 'Disconnected'\n",
    "        valid_rows = df[date].isin(['Active', 'Not Active', 'Disconnected'])\n",
    "        total_observed_stretch_length = df.loc[valid_rows, 'Stretch Length (km)'].sum()\n",
    "\n",
    "        # Calculate the percentage of ADNL based on the total stretch length\n",
    "        ADNL_p[date] = ADNL[date] / total_stretch_length if total_stretch_length > 0 else np.nan\n",
    "\n",
    "        # Calculate the percentage of ADNL based on the observed stretch length\n",
    "        ADNL_p_o[date] = ADNL[date] / total_observed_stretch_length if total_observed_stretch_length > 0 else np.nan\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    ANDL_df = pd.DataFrame({\n",
    "        'Survey Date': list(ADNL.keys()),\n",
    "        'Active Network Drainage Length': list(ADNL.values()),\n",
    "        'Percentage of Total Network': [ADNL_p[date] for date in ADNL.keys()],\n",
    "        'Percentage of Observed Network': [ADNL_p_o[date] for date in ADNL.keys()]\n",
    "    })\n",
    "\n",
    "    return ANDL_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0710dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the ADNL DataFrame for all survey dates\n",
    "ADNL_all = calculate_ADNL(stretches_data, survey_date)\n",
    "ADNL_all\n",
    "\n",
    "# Filter out rows where the percentage of observed network is not equal to the percentage of total network\n",
    "ADNL_df = ADNL_all[ADNL_all['Percentage of Observed Network'] == ADNL_all['Percentage of Total Network']].copy()\n",
    "ADNL_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e4a5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Drainage_ID' and calculate the max and min total length for each group\n",
    "drainage_length_stats = ADNL_df['Percentage of Total Network'].agg(['max', 'min', 'mean', 'median', 'std',])\n",
    "drainage_length_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2143d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNL_all.to_pickle('LCRW_ActiveNetworkDrainageLength.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735643e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where the observed percentage matches the total percentage; no missing information\n",
    "ADNL_LCRW = ADNL_all[ADNL_all['Percentage of Observed Network'] == ADNL_all['Percentage of Total Network']]\n",
    "\n",
    "ADNL_LCRW_copy = ADNL_LCRW.copy()\n",
    "ADNL_LCRW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103989c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the corresponding survey dates for valid data points\n",
    "survey_dates = ADNL_LCRW['Survey Date']\n",
    "survey_dates = pd.to_datetime(ADNL_LCRW['Survey Date'], format='%Y_%m_%d')\n",
    "survey_dates = survey_dates.dt.tz_localize('UTC')\n",
    "ADNL_LCRW = ADNL_LCRW.set_index('Survey Date')\n",
    "ADNL_LCRW.index.name = None\n",
    "ADNL_LCRW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847f8c6f",
   "metadata": {},
   "source": [
    "## Temporal Analysis: Relationship to Antecedent Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9bae5d",
   "metadata": {},
   "source": [
    "### Antecedent Precipitation & Dry Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfc0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_precip_LCRW = pd.read_parquet(Path(\"..\") / \"Data\" / \"Precipitation_24h_Jan24toJun25(UTC-6).parquet\")\n",
    "\n",
    "# To ensure it's a DataFrame, reset the index\n",
    "gridded_precip_LCRW_df = gridded_precip_LCRW.reset_index()\n",
    "\n",
    "# Rename the column 'timestamp' to 'datetime'\n",
    "gridded_precip_LCRW_df.rename(columns={'timestamp': 'datetime'}, inplace=True)\n",
    "\n",
    "# gridded_precip_LCRW_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e9b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a copy of the original precip data\n",
    "OG_precip = gridded_precip_LCRW.copy()\n",
    "\n",
    "# Make sure the index is datetime and sorted\n",
    "OG_precip.index = pd.to_datetime(OG_precip.index)\n",
    "OG_precip.sort_index(inplace=True)\n",
    "\n",
    "# Initialize the dry day counter for the LCRW column\n",
    "dry_days = []\n",
    "counter = 0\n",
    "\n",
    "# Count consecutive dry days BEFORE each date\n",
    "for precip in OG_precip['LCRW']:\n",
    "    dry_days.append(counter)\n",
    "    # if precip == 0:\n",
    "    if precip < 9.99:\n",
    "        counter += 1\n",
    "    else:\n",
    "        counter = 0\n",
    "\n",
    "# Add result as new column\n",
    "OG_precip['Dry_Days_Before_LCRW'] = dry_days\n",
    "\n",
    "# Strip time component from the OG_precip index for merging\n",
    "OG_precip = OG_precip.copy()\n",
    "OG_precip['Date'] = OG_precip.index.date  # datetime.date objects\n",
    "\n",
    "# Keep only relevant column(s)\n",
    "dry_days_df = OG_precip[['Date', 'Dry_Days_Before_LCRW']].drop_duplicates()\n",
    "dry_days_df = dry_days_df.set_index('Date')\n",
    "# dry_days_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d23130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_antecedent_precip(survey_dates, site_data, max_days=60):  # Note site_data is the df with the effective precipitation data for each day\n",
    "    \n",
    "    # Make a copy of site_data to avoid modifying the original DataFrame\n",
    "    site_data_copy = site_data.copy()\n",
    "\n",
    "    # Initialize an empty list to hold the results for the single site\n",
    "    results = {f'{survey_date}': [] for survey_date in survey_dates}\n",
    "\n",
    "    # Ensure 'datetime' is in the correct format and set it as the index in the copied DataFrame\n",
    "    site_data_copy['datetime'] = pd.to_datetime(site_data_copy['datetime'])  # Ensure the 'datetime' column is datetime\n",
    "    site_data_copy.set_index('datetime', inplace=True)  # Set 'datetime' as the index for easy comparison\n",
    "\n",
    "    # Loop over each survey date\n",
    "    for survey_date in survey_dates:\n",
    "        # Ensure survey_date is a datetime object\n",
    "        survey_date = pd.to_datetime(survey_date) if isinstance(survey_date, str) else survey_date\n",
    "\n",
    "        # Calculate antecedent effective precipitation for each window (1 to max_days)\n",
    "        for days in range(1, max_days + 1):\n",
    "           \n",
    "            start_date = survey_date - pd.Timedelta(days=days)\n",
    "           \n",
    "            # Filter data for the current site within the date window\n",
    "            site_data_in_window = site_data_copy.loc[(site_data_copy.index >= start_date) & (site_data_copy.index <= survey_date)]\n",
    "            \n",
    "            # Calculate the antecedent precipitation by summing the precipitation values in the window\n",
    "            antecedent_precip = site_data_in_window['LCRW'].sum() if not site_data_in_window.empty else None\n",
    "            \n",
    "            # Store the result in the dictionary with the key as the survey date\n",
    "            results[f'{survey_date}'].append(antecedent_precip)\n",
    "\n",
    "    # Convert the results dictionary into a DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "\n",
    "    # Set the index to be the time windows (1 to 60 days)\n",
    "    result_df.index = [f'{days}_days' for days in range(1, max_days + 1)]\n",
    "\n",
    "    return result_df\n",
    "\n",
    "Antecedent_Precip_LCRW = calculate_antecedent_precip(survey_dates, gridded_precip_LCRW_df)\n",
    "# Antecedent_Precip_LCRW.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66282182",
   "metadata": {},
   "source": [
    "### Antecedent Evapotranspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd5ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using API query in get_openet_polygon(simple).ipynb\n",
    "LCRW_ET_data = pd.read_parquet(Path(\"..\") / \"Data\" / \"ET_24h_Jun'25.parquet\")\n",
    "\n",
    "LCRW_ET_data.reset_index(inplace=True)\n",
    "# Rename the column 'timestamp' to 'datetime'\n",
    "LCRW_ET_data.rename(columns={'time': 'datetime'}, inplace=True)\n",
    "\n",
    "# Convert ET from in to mm\n",
    "LCRW_ET_data[\"et_mm\"] = LCRW_ET_data[\"et\"]*25.4\n",
    "LCRW_ET_data[\"datetime\"] = LCRW_ET_data[\"datetime\"].dt.date\n",
    "# LCRW_ET_data\n",
    "\n",
    "LCRW_ET_data['datetime'] = pd.to_datetime(LCRW_ET_data['datetime'])\n",
    "\n",
    "# Localize to the local timezone (e.g., 'America/New_York') and then convert to UTC\n",
    "LCRW_ET_data['datetime'] = LCRW_ET_data['datetime'].dt.tz_localize('America/New_York').dt.tz_convert('UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1e6d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_antecedent_ET(survey_dates, site_data, max_days=60):  # Note site_data is the df with the effective precipitation data for each day\n",
    "    \n",
    "    # Make a copy of site_data to avoid modifying the original DataFrame\n",
    "    site_data_copy = site_data.copy()\n",
    "\n",
    "    # Initialize an empty list to hold the results for the single site\n",
    "    results = {f'{survey_date}': [] for survey_date in survey_dates}\n",
    "\n",
    "    # Ensure 'datetime' is in the correct format and set it as the index in the copied DataFrame\n",
    "    site_data_copy['datetime'] = pd.to_datetime(site_data_copy['datetime'])  # Ensure the 'datetime' column is datetime\n",
    "    site_data_copy.set_index('datetime', inplace=True)  # Set 'datetime' as the index for easy comparison\n",
    "\n",
    "    # Loop over each survey date\n",
    "    for survey_date in survey_dates:\n",
    "        # Ensure survey_date is a datetime object\n",
    "        survey_date = pd.to_datetime(survey_date) if isinstance(survey_date, str) else survey_date\n",
    "\n",
    "        # Calculate antecedent effective precipitation for each window (1 to max_days)\n",
    "        for days in range(1, max_days + 1):\n",
    "           \n",
    "            start_date = survey_date - pd.Timedelta(days=days)\n",
    "           \n",
    "            # Filter data for the current site within the date window\n",
    "            site_data_in_window = site_data_copy.loc[(site_data_copy.index >= start_date) & (site_data_copy.index <= survey_date)]\n",
    "            \n",
    "            # Calculate the antecedent precipitation by summing the precipitation values in the window\n",
    "            antecedent_precip = site_data_in_window['et_mm'].sum() if not site_data_in_window.empty else None\n",
    "            \n",
    "            # Store the result in the dictionary with the key as the survey date\n",
    "            results[f'{survey_date}'].append(antecedent_precip)\n",
    "\n",
    "    # Convert the results dictionary into a DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "\n",
    "    # Set the index to be the time windows (1 to 60 days)\n",
    "    result_df.index = [f'{days}_days' for days in range(1, max_days + 1)]\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02bad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "Antecedent_ET_LCRW = calculate_antecedent_ET(survey_dates, LCRW_ET_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6eb9e7",
   "metadata": {},
   "source": [
    "### Antecedent Effective Precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21192911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize both datetime columns to only the date part\n",
    "gridded_precip_LCRW_df['date'] = gridded_precip_LCRW_df['datetime'].dt.date\n",
    "LCRW_ET_data['date'] = LCRW_ET_data['datetime'].dt.date\n",
    "\n",
    "# Merge the result with thiessen_precip_data on 'datetime'\n",
    "LCRW_EP_data = pd.merge(LCRW_ET_data[['date', 'et_mm']], \n",
    "                   gridded_precip_LCRW_df[['date', 'LCRW']], \n",
    "                   on='date', \n",
    "                   how='left')\n",
    "\n",
    "\n",
    "# Subtract the ET from Precip NOT Cumulative\n",
    "LCRW_EP_data[\"Effective Precipitation\"] = np.where(\n",
    "    LCRW_EP_data[\"LCRW\"] >= LCRW_EP_data[\"et_mm\"], \n",
    "    LCRW_EP_data[\"LCRW\"] - LCRW_EP_data[\"et_mm\"], \n",
    "    LCRW_EP_data[\"LCRW\"]\n",
    ")\n",
    "\n",
    "# Display the merged DataFrame\n",
    "# LCRW_EP_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb6671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' column to datetime (time will be set to 00:00:00)\n",
    "LCRW_EP_data['datetime'] = pd.to_datetime(LCRW_EP_data['date'])\n",
    "\n",
    "# Localize the datetime column to UTC\n",
    "LCRW_EP_data['datetime'] = LCRW_EP_data['datetime'].dt.tz_localize('UTC')\n",
    "\n",
    "# Verify the dtype is datetime64\n",
    "# print(LCRW_EP_data['datetime'].dtype)\n",
    "# LCRW_EP_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721d2620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_antecedent_EP(survey_dates, site_data, max_days=60):  # Note site_data is the df with the effective precipitation data for each day\n",
    "    \n",
    "    # Make a copy of site_data to avoid modifying the original DataFrame\n",
    "    site_data_copy = site_data.copy()\n",
    "\n",
    "    # Initialize an empty list to hold the results for the single site\n",
    "    results = {f'{survey_date}': [] for survey_date in survey_dates}\n",
    "\n",
    "    # Ensure 'datetime' is in the correct format and set it as the index in the copied DataFrame\n",
    "    site_data_copy['datetime'] = pd.to_datetime(site_data_copy['datetime'])  # Ensure the 'datetime' column is datetime\n",
    "    site_data_copy.set_index('datetime', inplace=True)  # Set 'datetime' as the index for easy comparison\n",
    "\n",
    "    # Loop over each survey date\n",
    "    for survey_date in survey_dates:\n",
    "        # Ensure survey_date is a datetime object\n",
    "        survey_date = pd.to_datetime(survey_date) if isinstance(survey_date, str) else survey_date\n",
    "\n",
    "        # Calculate antecedent effective precipitation for each window (1 to max_days)\n",
    "        for days in range(1, max_days + 1):\n",
    "           \n",
    "            start_date = survey_date - pd.Timedelta(days=days)\n",
    "           \n",
    "            # Filter data for the current site within the date window\n",
    "            site_data_in_window = site_data_copy.loc[(site_data_copy.index >= start_date) & (site_data_copy.index <= survey_date)]\n",
    "            \n",
    "            # Calculate the antecedent precipitation by summing the precipitation values in the window\n",
    "            antecedent_precip = site_data_in_window['Effective Precipitation'].sum() if not site_data_in_window.empty else None\n",
    "            \n",
    "            # Store the result in the dictionary with the key as the survey date\n",
    "            results[f'{survey_date}'].append(antecedent_precip)\n",
    "\n",
    "    # Convert the results dictionary into a DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "\n",
    "    # Set the index to be the time windows (1 to 60 days)\n",
    "    result_df.index = [f'{days}_days' for days in range(1, max_days + 1)]\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683d9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "Antecedent_EP_LCRW_1 = calculate_antecedent_EP(survey_dates, LCRW_EP_data)\n",
    "Antecedent_EP_LCRW_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5844cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "\n",
    "def calculate_correlations(ADNL_df, hydro_param_df, hydro_param_name, plot=False):\n",
    "    correlations = {}\n",
    "    model_params = {}\n",
    "\n",
    "    # Get the \"Active Network Drainage Length\" values as the dependent variable\n",
    "    y_cor = ADNL_df[\"Active Network Drainage Length\"].values\n",
    "\n",
    "    # Loop over each time window (1_day, 2_days, etc.) in hydro_param_df\n",
    "    for days, row in hydro_param_df.iterrows():\n",
    "        correlations[days] = {}  # Initialize an empty dictionary for each time window\n",
    "\n",
    "        # Extract the antecedent precipitation for this time window and remove NaNs\n",
    "        filtered = row.astype(float)\n",
    "        nan_mask = -np.isnan(filtered)  # Mask for NaN values\n",
    "        x_cor = filtered.values[nan_mask]  # Filter out NaN values in the x data (precipitation)\n",
    "        \n",
    "        # Perform linear regression: correlation between antecedent precipitation (x_cor) and Active Network Drainage Length (y_cor)\n",
    "        ling = linregress(x_cor, y_cor[nan_mask])\n",
    "        correlations[days] = ling  # Store the result for the current time window\n",
    "        \n",
    "        # Store slope and intercept for each time window in the model_params dictionary\n",
    "        model_params[days] = {'slope': ling.slope, 'intercept': ling.intercept}\n",
    "\n",
    "    # Print the maximum r-value and corresponding day(s)\n",
    "    max_rvalue = max([v.rvalue for v in correlations.values()])\n",
    "    max_rvalue_days = [days for days, v in correlations.items() if v.rvalue == max_rvalue]\n",
    "\n",
    "    print(f\"Maximum r-value: {max_rvalue:.3f} on day(s) {max_rvalue_days}\")\n",
    "\n",
    "    # Store the slope and intercept for the best time window (maximum r-value)\n",
    "    best_slope = model_params[max_rvalue_days[0]]['slope']\n",
    "    best_intercept = model_params[max_rvalue_days[0]]['intercept']\n",
    "    best_window = max_rvalue_days[0]\n",
    "\n",
    "    print(f\"Best model parameters for {max_rvalue_days[0]} day(s):\")\n",
    "    print(f\"Slope: {best_slope:.3f}, Intercept: {best_intercept:.3f}\")\n",
    "\n",
    "\n",
    "    # Plot the correlations if requested, but only once after processing all time windows\n",
    "    if plot:\n",
    "        fig, ax1 = plt.subplots()\n",
    "\n",
    "        # Extract r-values and p-values\n",
    "        r_values = [v.rvalue for v in correlations.values()]\n",
    "        p_values = [v.pvalue for v in correlations.values()]\n",
    "\n",
    "        # Plot r-values (correlation coefficients) on the left y-axis\n",
    "        ax1.plot(range(1, len(correlations) + 1), r_values, color='tab:blue', label='r-value')\n",
    "        ax1.set_xlabel(\"Window length [days]\")\n",
    "        ax1.set_ylabel(\"r-value\", color='tab:blue')\n",
    "        ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "        ax1.set_ylim(-1, 1)\n",
    "\n",
    "        # Highlight the max r-value with a vertical line and annotation\n",
    "        max_rvalue_day = r_values.index(max_rvalue) + 1  # +1 because days are 1-indexed\n",
    "        ax1.axvline(x=max_rvalue_day, color='tab:red', linestyle='--', linewidth=2, label='Max r-value')\n",
    "\n",
    "        ax1.annotate(f'Max r = {max_rvalue:.3f} on Day {max_rvalue_day}', \n",
    "                     xy=(max_rvalue_day, max_rvalue), \n",
    "                     xytext=(max_rvalue_day + 2, max_rvalue),  # Shift annotation to the right\n",
    "                     arrowprops=dict(arrowstyle=\"->\", lw=1.5),\n",
    "                     fontsize=10, color='tab:blue')\n",
    "\n",
    "        # Create a second y-axis to plot p-values\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(range(1, len(correlations) + 1), p_values, color='tab:red', label='p-value')\n",
    "        ax2.set_ylabel(\"p-value\", color='tab:red')\n",
    "        ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "        # Annotate the p-value at the maximum r-value day\n",
    "        pvalue_at_max_r = p_values[max_rvalue_day - 1]  # Get p-value at the max r-value day\n",
    "        ax2.annotate(f'p-value = {pvalue_at_max_r:.3f} on Day {max_rvalue_day}', \n",
    "                     xy=(max_rvalue_day, pvalue_at_max_r),  \n",
    "                     xytext=(max_rvalue_day + 2, pvalue_at_max_r - 0.025),  # Shift annotation to the right\n",
    "                     arrowprops=dict(arrowstyle=\"->\", lw=1.5),\n",
    "                     fontsize=10, color='tab:red')\n",
    "\n",
    "        # Add a title and tight layout for better readability\n",
    "        ax1.set_title(f\"Correlation for {hydro_param_name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    return correlations, best_window, best_slope, best_intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNL_EP1_correlations, ADNL_EP1_best_window, ADNL_EP1_best_slope, ADNL_EP1_best_intercept = calculate_correlations(ADNL_LCRW, Antecedent_EP_LCRW_1, \"Antecedent Cumulative Effective Precipitation\",True)\n",
    "# ADNL_EP1_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd47f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNL_Precip_correlations, ADNL_Precip_best_window, ADNL_Precip_best_slope, ADNL_Precip_best_intercept = calculate_correlations(ADNL_LCRW, Antecedent_Precip_LCRW, \"Antecedent Cumulative Precipitation\", True)\n",
    "# ADNL_Precip_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35bf24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNL_ET_correlations, ADNL_ET_best_window, ADNL_ET_best_slope, ADNL_ET_best_intercept = calculate_correlations(ADNL_LCRW, Antecedent_ET_LCRW, \"Antecedent Cumulative Evapotranspiration\" ,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save all correlations in a pickle file\n",
    "ADNL_Precip_correlations = calculate_correlations(ADNL_LCRW, Antecedent_Precip_LCRW, \"Antecedent Cumulative Precipitation\")\n",
    "with open(\"ADNL_Precip_correlations_LCRW.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ADNL_Precip_correlations, f)\n",
    "\n",
    "# Save all correlations in a pickle file\n",
    "ADNL_ET_correlations = calculate_correlations(ADNL_LCRW, Antecedent_ET_LCRW, \"Antecedent Cumulative Evapotranspiration\")\n",
    "with open(\"ADNL_ET_correlations_LCRW.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ADNL_ET_correlations, f)\n",
    "\n",
    "# Save all correlations in a pickle file\n",
    "ADNL_EP1_correlations = calculate_correlations(ADNL_LCRW, Antecedent_EP_LCRW_1, \"Antecedent Cumulative Effective Precipitation\")\n",
    "with open(\"ADNL_EP_correlations_LCRW.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ADNL_EP1_correlations, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b0b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the correlations from all the pickle files\n",
    "with open(\"ADNL_Precip_correlations_LCRW.pkl\", \"rb\") as f:\n",
    "    ADNL_Precip_correlations_LCRW = pickle.load(f)\n",
    "# Load the correlations from the pickle file\n",
    "with open(\"ADNL_ET_correlations_LCRW.pkl\", \"rb\") as f:\n",
    "    ADNL_ET_correlations_LCRW = pickle.load(f)\n",
    "# Load the correlations from the pickle file\n",
    "with open(\"ADNL_EP_correlations_LCRW.pkl\", \"rb\") as f:\n",
    "    ADNL_EP1_correlations_LCRW = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd064d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a dictionary for each dataset and its corresponding correlation data for all 4 types\n",
    "datasets = {\n",
    "    'LCRW': {\n",
    "        'Precip': ADNL_Precip_correlations_LCRW,\n",
    "        'ET': ADNL_ET_correlations_LCRW,\n",
    "        'EP1': ADNL_EP1_correlations_LCRW\n",
    "    },\n",
    "}\n",
    "\n",
    "def get_rvalues(dataset_name, correlation_type):\n",
    "    # Assuming it's a tuple and the first element contains the dictionary\n",
    "    correlation_data = datasets[dataset_name][correlation_type][0] \n",
    "    return [correlation_data[f'{day}_days'].rvalue for day in range(1, 60)]\n",
    "\n",
    "# Define the dataset you're using\n",
    "dataset_name = 'LCRW'\n",
    "\n",
    "correlation_types = ['Precip', 'EP1']\n",
    "label_names = {'Precip': 'Precipitation', 'EP1': 'Effective Precipitation'}\n",
    "colors_by_type = {'Precip': 'royalblue', 'ET': 'orange', 'EP1': 'tomato'}\n",
    "time_windows = range(1, 61)  # 1–60 days\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "\n",
    "for corr_type in correlation_types:\n",
    "    rvalues = get_rvalues(dataset_name, corr_type)\n",
    "\n",
    "    # Truncate if needed to avoid length mismatch\n",
    "    min_len = min(len(time_windows), len(rvalues))\n",
    "    rvalues = rvalues[:min_len]\n",
    "    time_range = list(time_windows)[:min_len]\n",
    "\n",
    "    plt.plot(time_range, rvalues, label=label_names[corr_type],\n",
    "             linestyle='-',\n",
    "             color=colors_by_type[corr_type])\n",
    "\n",
    "    # Annotate max point\n",
    "    max_r = max(rvalues)\n",
    "    max_day = rvalues.index(max_r) + 1\n",
    "    plt.annotate(f'Max r = {max_r:.3f} at {max_day} days',\n",
    "                 xy=(max_day, max_r),\n",
    "                 xytext=(max_day + 3, max_r - 0.02),\n",
    "                 arrowprops=dict(arrowstyle=\"->\"),\n",
    "                 fontsize=9,\n",
    "                 color=colors_by_type[corr_type])\n",
    "\n",
    "plt.xlim(0, 60)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xlabel('Antecedent Window Length (Days)')\n",
    "plt.ylabel('Pearson Correlation (r)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cbe0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated to retrieve both r-values and p-values\n",
    "def get_r_and_p_values(dataset_name, correlation_type):\n",
    "    correlation_data = datasets[dataset_name][correlation_type][0]\n",
    "    r_values = [correlation_data[f'{day}_days'].rvalue for day in range(1, 60)]\n",
    "    p_values = [correlation_data[f'{day}_days'].pvalue for day in range(1, 60)]\n",
    "    return r_values, p_values\n",
    "\n",
    "\n",
    "correlation_types = ['Precip', 'EP1']\n",
    "label_names = {'Precip': 'Precipitation', 'EP1': 'Effective Precipitation'}\n",
    "colors_by_type = {'Precip': 'royalblue', 'ET': 'orange', 'EP1': 'tomato'}\n",
    "time_windows = range(1, 61)  # 1–60 days\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "for corr_type in correlation_types:\n",
    "    r_values, p_values = get_r_and_p_values('LCRW', corr_type)\n",
    "\n",
    "    min_len = min(len(time_windows), len(r_values))\n",
    "    time_range = list(time_windows)[:min_len]\n",
    "    r_values = r_values[:min_len]\n",
    "    p_values = p_values[:min_len]\n",
    "\n",
    "    ax1.plot(time_range, r_values, label=label_names[corr_type],\n",
    "             linestyle='-', color=colors_by_type[corr_type])\n",
    "\n",
    "    # Annotate the max r-value\n",
    "    max_r = max(r_values)\n",
    "    max_day = r_values.index(max_r) + 1\n",
    "    ax1.annotate(f'Max r = {max_r:.3f} at {max_day} days',\n",
    "                 xy=(max_day, max_r),\n",
    "                 xytext=(max_day + 3, max_r - 0.02),\n",
    "                 arrowprops=dict(arrowstyle=\"->\"),\n",
    "                 fontsize=9,\n",
    "                 color=colors_by_type[corr_type])\n",
    "\n",
    "# Set axis limits and labels for r-values\n",
    "ax1.set_xlim(0, 60)\n",
    "ax1.set_ylim(0, 1.0)\n",
    "ax1.set_xlabel('Antecedent Window Length (Days)')\n",
    "ax1.set_ylabel('Pearson Correlation (r)')\n",
    "ax1.grid(True)\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Create secondary y-axis for p-values\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "for corr_type in correlation_types:\n",
    "    _, p_values = get_r_and_p_values('LCRW', corr_type)\n",
    "    p_values = p_values[:60]\n",
    "    ax2.plot(time_range, p_values, linestyle='--',\n",
    "             color=colors_by_type[corr_type], alpha=0.6)\n",
    "\n",
    "# Add horizontal dotted line for p = 0.001\n",
    "ax2.axhline(y=0.01, color='gray', linestyle=':', linewidth=1.2, label='p = 0.01 (significance)')\n",
    "\n",
    "ax2.set_ylabel(\"p-value\", color='black')\n",
    "ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "# Optional: add a combined legend\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines + lines2, labels , loc='upper right', ncol=1)\n",
    "\n",
    "ax2.text(61, 0.01, 'p = 0.01', \n",
    "         va='center', ha='left', fontsize=10, color='gray')\n",
    "\n",
    "      \n",
    "\n",
    "plt.title('Correlation and p-values vs. Antecedent Window Length')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9145a7",
   "metadata": {},
   "source": [
    "# Predictive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda4cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_antecedent_EP_best_window(start_date, end_date, site_data, best_window):\n",
    "    \"\"\"\n",
    "    Calculate antecedent precipitation for each day between start_date and end_date using the best time window.\n",
    "    Ensures consistency with the other function's method (with 6-day window and inclusive date range).\n",
    "\n",
    "    Parameters:\n",
    "    - start_date: The start date of the prediction period.\n",
    "    - end_date: The end date of the prediction period.\n",
    "    - site_data: DataFrame containing precipitation data with 'datetime' and 'Effective Precipitation' columns.\n",
    "    - best_window: The best time window for antecedent precipitation (in days, format '6_days').\n",
    "\n",
    "    Returns:\n",
    "    - result_df: DataFrame with antecedent precipitation for each survey date and the best window length.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a copy of site_data to avoid modifying the original DataFrame\n",
    "    site_data_copy = site_data.copy()\n",
    "\n",
    "    # Ensure 'datetime' is in the correct format and set it as the index\n",
    "    site_data_copy['datetime'] = pd.to_datetime(site_data_copy['datetime'])\n",
    "\n",
    "    # If datetime is timezone-aware, make it timezone-naive\n",
    "    if site_data_copy['datetime'].dt.tz is not None:\n",
    "        site_data_copy['datetime'] = site_data_copy['datetime'].dt.tz_localize(None)\n",
    "    \n",
    "    site_data_copy.set_index('datetime', inplace=True)\n",
    "    \n",
    "    # Convert start_date and end_date to datetime, and make them timezone-naive if they are not already\n",
    "    start_date = pd.to_datetime(start_date).normalize()  # Removing time component for comparison\n",
    "    end_date = pd.to_datetime(end_date).normalize()\n",
    "\n",
    "    # Extract the numerical value for the best window (e.g., '6_days' -> 6)\n",
    "    best_window_days = int(best_window.split('_')[0])\n",
    "\n",
    "    # Generate the list of survey dates (from start_date to end_date)\n",
    "    survey_dates = pd.date_range(start=start_date, end=end_date)\n",
    "    \n",
    "    # Initialize a list to hold the results\n",
    "    results = []\n",
    "\n",
    "    # Loop through each survey date\n",
    "    for survey_date in survey_dates:\n",
    "        # Ensure survey_date is a datetime object\n",
    "        survey_date = pd.to_datetime(survey_date)\n",
    "\n",
    "        # Calculate the start date for the antecedent precipitation window (inclusive of start_date_window)\n",
    "        start_date_window = survey_date - pd.Timedelta(days=best_window_days)\n",
    "\n",
    "        # Filter data for the current site within the best time window (inclusive start_date_window to survey_date)\n",
    "        site_data_in_window = site_data_copy.loc[(site_data_copy.index >= start_date_window) & (site_data_copy.index <= survey_date)]\n",
    "\n",
    "        # Calculate the antecedent precipitation by summing the precipitation values in the window\n",
    "        antecedent_precip = site_data_in_window['Effective Precipitation'].sum() if not site_data_in_window.empty else None\n",
    "\n",
    "        # Append the result to the list\n",
    "        results.append([survey_date, antecedent_precip])\n",
    "\n",
    "    # Convert the results into a DataFrame\n",
    "    result_df = pd.DataFrame(results, columns=['Survey_Date', 'Antecedent_EffectivePrecip'])\n",
    "\n",
    "    # Set the index to be the survey dates for better readability\n",
    "    result_df.set_index('Survey_Date', inplace=True)\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf81ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Antecedent_EP_bestwindow =  calculate_antecedent_EP_best_window('05-01-2024', '06-29-2025', LCRW_EP_data, ADNL_EP1_best_window)\n",
    "Antecedent_EP_dry = Antecedent_EP_bestwindow.copy()\n",
    "# Reset Pandas display options to default\n",
    "pd.reset_option('display.max_columns')\n",
    "pd.reset_option('display.max_rows')\n",
    "# Antecedent_EP_bestwindow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625ab567",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_max = 71.28007300000002\n",
    "\n",
    "Antecedent_EP_bestwindow['Predicted ADNL_Linear'] = Antecedent_EP_bestwindow['Antecedent_EffectivePrecip']*ADNL_EP1_best_slope + ADNL_EP1_best_intercept\n",
    "Antecedent_EP_bestwindow['Predicted ADNL_Linear_capped'] = np.minimum(Antecedent_EP_bestwindow['Antecedent_EffectivePrecip']*ADNL_EP1_best_slope + ADNL_EP1_best_intercept, L_max)\n",
    "print(ADNL_EP1_best_slope, ADNL_EP1_best_intercept)\n",
    "Antecedent_EP_bestwindow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb9ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Set Survey Date as the index to align on date\n",
    "actual_series = ADNL_LCRW_copy.set_index('Survey Date')['Active Network Drainage Length']\n",
    "predicted_series_EP = Antecedent_EP_bestwindow['Predicted ADNL_Linear']\n",
    "\n",
    "# Convert index with underscores to datetime by first replacing '_' with '-'\n",
    "actual_series.index = pd.to_datetime(actual_series.index.str.replace('_', '-'))\n",
    "\n",
    "# Align both series on the index (i.e., matching dates)\n",
    "actual_aligned, predicted_aligned = actual_series.align(predicted_series_EP, join='inner')\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(actual_aligned, predicted_aligned)\n",
    "mse = mean_squared_error(actual_aligned, predicted_aligned)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(actual_aligned, predicted_aligned)\n",
    "\n",
    "# Calculate Pearson correlation coefficient (R)\n",
    "r_value, p_value = pearsonr(actual_aligned, predicted_aligned)\n",
    "\n",
    "# Output the results\n",
    "print(f\"----For the EP 6 Linear Model----\")\n",
    "print(f\"MAE: {mae:.3f}\")\n",
    "print(f\"MSE: {mse:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"R² Score: {r2:.3f}\")\n",
    "print(f\"R (Pearson correlation): {r_value:.3f}\")\n",
    "print(f\"p-value: {p_value:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79042e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Set Survey Date as the index to align on date\n",
    "actual_series = ADNL_LCRW_copy.set_index('Survey Date')['Active Network Drainage Length']\n",
    "predicted_series_EP_capped = Antecedent_EP_bestwindow['Predicted ADNL_Linear_capped']\n",
    "\n",
    "# Convert index with underscores to datetime by first replacing '_' with '-'\n",
    "actual_series.index = pd.to_datetime(actual_series.index.str.replace('_', '-'))\n",
    "\n",
    "# Align both series on the index (i.e., matching dates)\n",
    "actual_aligned, predicted_aligned_capped = actual_series.align(predicted_series_EP_capped, join='inner')\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(actual_aligned, predicted_aligned_capped)\n",
    "mse = mean_squared_error(actual_aligned, predicted_aligned_capped)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(actual_aligned, predicted_aligned_capped)\n",
    "\n",
    "# Calculate Pearson correlation coefficient (R)\n",
    "r_value, p_value = pearsonr(actual_aligned, predicted_aligned_capped)\n",
    "\n",
    "# Output the results\n",
    "print(f\"----For the EP 6 Capped Linear Model----\")\n",
    "print(f\"MAE: {mae:.3f}\")\n",
    "print(f\"MSE: {mse:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"R² Score: {r2:.3f}\")\n",
    "print(f\"R (Pearson correlation): {r_value:.3f}\")\n",
    "print(f\"p-value: {p_value:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b42f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "EP_df = Antecedent_EP_bestwindow['Antecedent_EffectivePrecip']\n",
    "\n",
    "# Combine into a DataFrame based on index\n",
    "df = pd.concat([\n",
    "    actual_aligned.rename('Actual'),\n",
    "    predicted_aligned.rename('Predicted'),\n",
    "    predicted_aligned_capped.rename('PredictedCapped'),\n",
    "    Antecedent_EP_bestwindow['Antecedent_EffectivePrecip'].rename('Antecedent_EffectivePrecip')\n",
    "], axis=1).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0079d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Survey Date' and 'Predicted_ADNL['date']' are naive datetime objects\n",
    "ADNL_LCRW_copy['Survey Date'] = pd.to_datetime(ADNL_LCRW_copy['Survey Date'], format='%Y_%m_%d').dt.tz_localize(None)\n",
    "\n",
    "# # Remove timezone info from the index of Antecedent_Precip_bestwindow\n",
    "# Antecedent_Precip_bestwindow.index = Antecedent_Precip_bestwindow.index.tz_localize(None)\n",
    "\n",
    "# Remove timezone info from the index of Antecedent_EP_bestwindow\n",
    "Antecedent_EP_bestwindow.index = Antecedent_EP_bestwindow.index.tz_localize(None)\n",
    "\n",
    "# Create figure and primary axis\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot Predicted ADNL (EP) on primary y-axis\n",
    "line1, = ax1.plot(\n",
    "    Antecedent_EP_bestwindow.index,\n",
    "    Antecedent_EP_bestwindow['Predicted ADNL_Linear_capped'],\n",
    "    color='orchid', linestyle='--', label='Predicted ADNL (EP)', zorder=2\n",
    ")\n",
    "\n",
    "# Plot Observed ADNL on same primary y-axis\n",
    "line2, = ax1.plot(\n",
    "    ADNL_LCRW_copy['Survey Date'],\n",
    "    ADNL_LCRW_copy['Active Network Drainage Length'],\n",
    "    marker='o', linestyle='', color='blue', label='Observed ADNL', zorder=3\n",
    ")\n",
    "\n",
    "# Set label for the y-axis\n",
    "ax1.set_ylabel('Active Network Drainage Length (km)', color='black')\n",
    "\n",
    "# Add third axis for Precipitation\n",
    "ax3 = ax1.twinx()\n",
    "ax3.spines['right'].set_position(('outward', 10))  # Offset to avoid overlap\n",
    "bar3 = ax3.bar(\n",
    "    gridded_precip_LCRW.index,\n",
    "    -gridded_precip_LCRW['LCRW'],\n",
    "    width=1, align='center', alpha=0.5, color='black',\n",
    "    label='Precipitation', zorder=1\n",
    ")\n",
    "ax3.set_ylabel('Precipitation (mm)', color='black')\n",
    "\n",
    "# Set y-axis limits\n",
    "ax1.set_ylim(50, 85)     # For ADNL\n",
    "ax3.set_ylim(-100, 0)    # For precipitation\n",
    "\n",
    "# Set x-axis limits\n",
    "start_date = pd.to_datetime('2024-05-01')\n",
    "end_date = pd.to_datetime('2024-11-30')\n",
    "ax1.set_xlim(start_date, end_date)\n",
    "ax3.set_xlim(start_date, end_date)\n",
    "\n",
    "# Add gridlines\n",
    "ax1.grid(True, which='both', linestyle='--', color='gray', alpha=0.5)\n",
    "\n",
    "# Combine all legend handles/labels\n",
    "handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "handles3, labels3 = ax3.get_legend_handles_labels()\n",
    "\n",
    "# Add unified legend below the plot\n",
    "fig.legend(\n",
    "    handles1 + handles3,\n",
    "    labels1 + labels3,\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 0.1),\n",
    "    ncol=3,\n",
    "    frameon=True\n",
    ")\n",
    "\n",
    "# Adjust layout to make room for legend\n",
    "plt.tight_layout(rect=[0, 0.08, 1, 1])\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b03d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on the index (date)\n",
    "Antecedent_EP_dry = Antecedent_EP_dry.merge(dry_days_df, left_index=True, right_index=True, how='left')\n",
    "Antecedent_EP_dry.reset_index(inplace=True)\n",
    "Antecedent_EP_dry\n",
    "# Rename 'Survey Date' in ADNL to match 'Survey_Date' in Antecedent_EP_dry\n",
    "ADNL_LCRW_copy.rename(columns={'Survey Date': 'Survey_Date'}, inplace=True)\n",
    "\n",
    "# Now merge on 'Survey_Date'\n",
    "Antecedent_EP_dry = Antecedent_EP_dry.merge(\n",
    "    ADNL_LCRW_copy[['Survey_Date', 'Active Network Drainage Length']],\n",
    "    on='Survey_Date',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "Antecedent_EP_dry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop rows with missing ADNL values\n",
    "Antecedent_EP_dry_clean = Antecedent_EP_dry.dropna(subset=['Active Network Drainage Length'])\n",
    "Antecedent_EP_dry_clean.to_pickle(r'C:\\Users\\mjg9941\\Documents\\PhD_Research\\Projects\\SmartWater\\Python_Projects\\Field_Campaigns\\dataretrieval\\Antecedent_EP_dry_clean.pkl')\n",
    "Antecedent_EP_dry_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Assume df is your DataFrame with no missing values\n",
    "df_dry = Antecedent_EP_dry_clean\n",
    "\n",
    "# Rename for clarity\n",
    "df_dry = df_dry.rename(columns={\n",
    "    'Antecedent_EffectivePrecip': 'EP',\n",
    "    'Dry_Days_Before_LCRW': 'DryDays',\n",
    "    'Active Network Drainage Length': 'ADNL'\n",
    "})\n",
    "\n",
    "# Construct custom design matrix: [EP, -DryDays]\n",
    "X = df_dry[['EP', 'DryDays']].copy()\n",
    "X['DryDays'] = -X['DryDays']  # Enforce subtraction\n",
    "\n",
    "y = df_dry['ADNL']\n",
    "\n",
    "# Fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get model coefficients\n",
    "x1, x2 = model.coef_\n",
    "b = model.intercept_\n",
    "\n",
    "# Predict and apply cap\n",
    "y_pred_raw = model.predict(X)\n",
    "y_pred_capped = np.minimum(y_pred_raw, L_max)\n",
    "\n",
    "# Print fitted equation\n",
    "print(f\"Fitted model: ADNL = ({x1:.4f} * EP) + ({x2:.4f} * DryDays) + {b:.4f}\")\n",
    "print(f\"Capping predictions at ADNL_max = {L_max:.4f}\")\n",
    "\n",
    "# Evaluate model performance using capped predictions\n",
    "r2 = r2_score(y, y_pred_capped)\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred_capped))\n",
    "mae = mean_absolute_error(y, y_pred_capped)\n",
    "\n",
    "# Step 4: Calculate Pearson correlation coefficient (R)\n",
    "r_value, p_value = pearsonr(y, y_pred_capped)\n",
    "\n",
    "print(f\"\\nModel Performance (Capped):\")\n",
    "print(f\"----For the EP 6 Multiparameter Model----\")\n",
    "print(f\"R²:   {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"R:    {r_value:.4f}\")\n",
    "print(f\"p:    {p_value:.10f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6ef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "# Apply formula and cap each predicted value at L_max\n",
    "Antecedent_EP_dry['Predicted_ADNL'] = np.minimum(\n",
    "    (x1 * Antecedent_EP_dry['Antecedent_EffectivePrecip']) +\n",
    "    (x2 * Antecedent_EP_dry['Dry_Days_Before_LCRW']) +\n",
    "    b,\n",
    "    L_max\n",
    ")\n",
    "\n",
    "# Set index to Survey_Date\n",
    "Antecedent_EP_dry.index = Antecedent_EP_dry['Survey_Date']\n",
    "\n",
    "Antecedent_EP_dry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246fa98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADNL_LCRW_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b4b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Survey Date parsing (no need for format if it's already YYYY-MM-DD)\n",
    "ADNL_LCRW_copy['Survey_Date'] = pd.to_datetime(ADNL_LCRW_copy['Survey_Date']).dt.tz_localize(None)\n",
    "\n",
    "# Ensure predicted series indices are datetime and tz-naive\n",
    "Antecedent_EP_dry.index = pd.to_datetime(Antecedent_EP_dry.index).tz_localize(None)\n",
    "Antecedent_EP_bestwindow.index = pd.to_datetime(Antecedent_EP_bestwindow.index).tz_localize(None)\n",
    "gridded_precip_LCRW.index = pd.to_datetime(gridded_precip_LCRW.index).tz_localize(None)\n",
    "\n",
    "# Plot\n",
    "fig, ax1 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Observed ADNL\n",
    "line_obs, = ax1.plot(\n",
    "    ADNL_LCRW_copy['Survey_Date'], ADNL_LCRW_copy['Active Network Drainage Length'],\n",
    "    marker='o', linestyle='', color='blue', label='Observed ADNL', zorder=3\n",
    ")\n",
    "\n",
    "# Predicted (Multilinear)\n",
    "line_multi, = ax1.plot(\n",
    "    Antecedent_EP_dry.index, Antecedent_EP_dry['Predicted_ADNL'],\n",
    "    color='tomato', linestyle='-', label='Multilinear Model', zorder=2\n",
    ")\n",
    "\n",
    "# Predicted (Linear - capped)\n",
    "line_lin_cap, = ax1.plot(\n",
    "    Antecedent_EP_bestwindow.index, Antecedent_EP_bestwindow['Predicted ADNL_Linear_capped'],\n",
    "    color='orchid', linestyle='--', label='Linear Model (Capped)', zorder=2\n",
    ")\n",
    "\n",
    "# Predicted (Linear - uncapped)\n",
    "line_lin_raw, = ax1.plot(\n",
    "    Antecedent_EP_bestwindow.index, Antecedent_EP_bestwindow['Predicted ADNL_Linear'],\n",
    "    color='mediumseagreen', linestyle='-.', label='Linear Model (Raw)', zorder=2\n",
    ")\n",
    "\n",
    "# Y-axis ADNL\n",
    "ax1.set_ylabel('Active Network Drainage Length (km)', color='black')\n",
    "ax1.set_ylim(50, 85)\n",
    "\n",
    "# Add precipitation on secondary axis\n",
    "ax3 = ax1.twinx()\n",
    "ax3.spines['right'].set_position(('outward', 10))\n",
    "bar_precip = ax3.bar(\n",
    "    gridded_precip_LCRW.index,\n",
    "    -np.clip(gridded_precip_LCRW['LCRW'], None, 45),\n",
    "    width=1, align='center', alpha=0.5, color='black',\n",
    "    label='Precipitation', zorder=1\n",
    ")\n",
    "ax3.set_ylabel('Precipitation (mm)', color='black')\n",
    "ax3.set_ylim(-100, 0)\n",
    "ax3.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{-int(x)}\"))\n",
    "\n",
    "# Date range\n",
    "start_date = pd.to_datetime('2024-05-01')\n",
    "end_date = pd.to_datetime('2024-12-01')\n",
    "ax1.set_xlim(start_date, end_date)\n",
    "ax3.set_xlim(start_date, end_date)\n",
    "\n",
    "# Grid and layout\n",
    "ax1.grid(True, which='both', linestyle='-', color='gray', alpha=0.5)\n",
    "plt.title(\"Observed vs Predicted ADNL with Precipitation\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "# # Combined legend\n",
    "# handles, labels = ax1.get_legend_handles_labels()\n",
    "# handles2, labels2 = ax3.get_legend_handles_labels()\n",
    "# ax1.legend(handles + handles2, labels + labels2, loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=2)\n",
    "\n",
    "# Show\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef9c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Survey Date parsing (no need for format if it's already YYYY-MM-DD)\n",
    "ADNL_LCRW_copy['Survey_Date'] = pd.to_datetime(ADNL_LCRW_copy['Survey_Date']).dt.tz_localize(None)\n",
    "\n",
    "# Ensure predicted series indices are datetime and tz-naive\n",
    "Antecedent_EP_dry.index = pd.to_datetime(Antecedent_EP_dry.index).tz_localize(None)\n",
    "Antecedent_EP_bestwindow.index = pd.to_datetime(Antecedent_EP_bestwindow.index).tz_localize(None)\n",
    "gridded_precip_LCRW.index = pd.to_datetime(gridded_precip_LCRW.index).tz_localize(None)\n",
    "\n",
    "# Plot\n",
    "fig, ax1 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Observed ADNL\n",
    "line_obs, = ax1.plot(\n",
    "    ADNL_LCRW_copy['Survey_Date'], ADNL_LCRW_copy['Active Network Drainage Length'],\n",
    "    marker='o', linestyle='', color='blue', label='Observed ADNL', zorder=3\n",
    ")\n",
    "\n",
    "# Predicted (Multilinear)\n",
    "line_multi, = ax1.plot(\n",
    "    Antecedent_EP_dry.index, Antecedent_EP_dry['Predicted_ADNL'],\n",
    "    color='tomato', linestyle='-', label='Multilinear Model', zorder=2\n",
    ")\n",
    "\n",
    "# # Predicted (Linear - capped)\n",
    "# line_lin_cap, = ax1.plot(\n",
    "#     Antecedent_EP_bestwindow.index, Antecedent_EP_bestwindow['Predicted ADNL_Linear_capped'],\n",
    "#     color='orchid', linestyle='--', label='Linear Model (Capped)', zorder=2\n",
    "# )\n",
    "\n",
    "# # Predicted (Linear - uncapped)\n",
    "# line_lin_raw, = ax1.plot(\n",
    "#     Antecedent_EP_bestwindow.index, Antecedent_EP_bestwindow['Predicted ADNL_Linear'],\n",
    "#     color='mediumseagreen', linestyle='-.', label='Linear Model (Raw)', zorder=2\n",
    "# )\n",
    "\n",
    "# Y-axis ADNL\n",
    "ax1.set_ylabel('Active Network Drainage Length (km)', color='black')\n",
    "ax1.set_ylim(50, 85)\n",
    "\n",
    "# Add precipitation on secondary axis\n",
    "ax3 = ax1.twinx()\n",
    "ax3.spines['right'].set_position(('outward', 10))\n",
    "bar_precip = ax3.bar(\n",
    "    gridded_precip_LCRW.index,\n",
    "    -np.clip(gridded_precip_LCRW['LCRW'], None, 45),\n",
    "    width=1, align='center', alpha=0.5, color='black',\n",
    "    label='Precipitation', zorder=1\n",
    ")\n",
    "ax3.set_ylabel('Precipitation (mm)', color='black')\n",
    "ax3.set_ylim(-100, 0)\n",
    "ax3.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{-int(x)}\"))\n",
    "\n",
    "# Date range\n",
    "start_date = pd.to_datetime('2024-05-01')\n",
    "end_date = pd.to_datetime('2024-12-01')\n",
    "ax1.set_xlim(start_date, end_date)\n",
    "ax3.set_xlim(start_date, end_date)\n",
    "\n",
    "# Grid and layout\n",
    "ax1.grid(True, which='both', linestyle='-', color='gray', alpha=0.5)\n",
    "# plt.title(\"Observed vs Predicted ADNL with Precipitation\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "# # Combined legend\n",
    "# handles, labels = ax1.get_legend_handles_labels()\n",
    "# handles2, labels2 = ax3.get_legend_handles_labels()\n",
    "# ax1.legend(handles + handles2, labels + labels2, loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=2)\n",
    "\n",
    "# Show\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56805376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: reindex all to match observed dates\n",
    "observed = ADNL_all.set_index(pd.to_datetime(ADNL_all['Survey Date'], format='%Y_%m_%d'))['Active Network Drainage Length']\n",
    "ep_multilinear = Antecedent_EP_dry['Predicted_ADNL'].reindex(observed.index).dropna()\n",
    "ep_linear = Antecedent_EP_bestwindow['Predicted ADNL_Linear'].reindex(observed.index).dropna()\n",
    "ep_capped_linear = Antecedent_EP_bestwindow['Predicted ADNL_Linear_capped'].reindex(observed.index).dropna()\n",
    "\n",
    "# Align all three with observed for shared dates\n",
    "shared_index = observed.index.intersection(ep_multilinear.index).intersection(ep_linear.index)\n",
    "observed = observed.loc[shared_index]\n",
    "ep_multilinear = ep_multilinear.loc[shared_index]\n",
    "ep_linear = ep_linear.loc[shared_index]\n",
    "ep_capped_linear = ep_capped_linear.loc[shared_index]\n",
    "\n",
    "# Filter for ADNL < 55\n",
    "mask_below_55 = observed < 55\n",
    "observed_below_55 = observed[mask_below_55]\n",
    "ep_multilinear_below_55 = ep_multilinear[mask_below_55]\n",
    "ep_linear_below_55 = ep_linear[mask_below_55]\n",
    "ep_capped_linear_below_55 = ep_capped_linear[mask_below_55]\n",
    "\n",
    "# RMSE calculation\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "# Print RMSEs\n",
    "print(\"RMSE below 55 km:\")\n",
    "print(\"Multilinear (EP):\", rmse(observed_below_55, ep_multilinear_below_55))\n",
    "print(\"Linear (EP):\", rmse(observed_below_55, ep_linear_below_55))\n",
    "print(\"Capped (EP):\", rmse(observed_below_55, ep_capped_linear_below_55))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_FC (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
